{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083df0b1-3535-4691-8b8a-74d006389c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f5a9b39-0ca0-4366-b604-a272eac0d54f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gini'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msplitter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'best'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "A decision tree classifier.\n",
       "\n",
       "Read more in the :ref:`User Guide <tree>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
       "    The function to measure the quality of a split. Supported criteria are\n",
       "    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
       "    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
       "\n",
       "splitter : {\"best\", \"random\"}, default=\"best\"\n",
       "    The strategy used to choose the split at each node. Supported\n",
       "    strategies are \"best\" to choose the best split and \"random\" to choose\n",
       "    the best random split.\n",
       "\n",
       "max_depth : int, default=None\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "        - If int, then consider `max_features` features at each split.\n",
       "        - If float, then `max_features` is a fraction and\n",
       "          `int(max_features * n_features)` features are considered at each\n",
       "          split.\n",
       "        - If \"auto\", then `max_features=sqrt(n_features)`.\n",
       "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "        - If \"log2\", then `max_features=log2(n_features)`.\n",
       "        - If None, then `max_features=n_features`.\n",
       "\n",
       "        .. deprecated:: 1.1\n",
       "            The `\"auto\"` option was deprecated in 1.1 and will be removed\n",
       "            in 1.3.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the randomness of the estimator. The features are always\n",
       "    randomly permuted at each split, even if ``splitter`` is set to\n",
       "    ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
       "    select ``max_features`` at random at each split before finding the best\n",
       "    split among them. But the best found split may vary across different\n",
       "    runs, even if ``max_features=n_features``. That is the case, if the\n",
       "    improvement of the criterion is identical for several splits and one\n",
       "    split has to be selected at random. To obtain a deterministic behaviour\n",
       "    during fitting, ``random_state`` has to be fixed to an integer.\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "class_weight : dict, list of dict or \"balanced\", default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If None, all classes are supposed to have weight one. For\n",
       "    multi-output problems, a list of dicts can be provided in the same\n",
       "    order as the columns of y.\n",
       "\n",
       "    Note that for multioutput (including multilabel) weights should be\n",
       "    defined for each class of every column in its own dict. For example,\n",
       "    for four-class multilabel classification weights should be\n",
       "    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
       "    [{1:1}, {2:5}, {3:1}, {4:1}].\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``\n",
       "\n",
       "    For multi-output, the weights of each column of y will be multiplied.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
       "    The classes labels (single output problem),\n",
       "    or a list of arrays of class labels (multi-output problem).\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance [4]_.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "max_features_ : int\n",
       "    The inferred value of max_features.\n",
       "\n",
       "n_classes_ : int or list of int\n",
       "    The number of classes (for single output problems),\n",
       "    or a list containing the number of classes for each\n",
       "    output (for multi-output problems).\n",
       "\n",
       "n_features_ : int\n",
       "    The number of features when ``fit`` is performed.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "       `n_features_` is deprecated in 1.0 and will be removed in\n",
       "       1.2. Use `n_features_in_` instead.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs when ``fit`` is performed.\n",
       "\n",
       "tree_ : Tree instance\n",
       "    The underlying Tree object. Please refer to\n",
       "    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
       "    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
       "    for basic usage of these attributes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DecisionTreeRegressor : A decision tree regressor.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
       "function on the outputs of :meth:`predict_proba`. This means that in\n",
       "case the highest predicted probabilities are tied, the classifier will\n",
       "predict the tied class with the lowest index in :term:`classes_`.\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       ".. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
       "\n",
       ".. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
       "       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
       "\n",
       ".. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
       "       Learning\", Springer, 2009.\n",
       "\n",
       ".. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
       "       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_iris\n",
       ">>> from sklearn.model_selection import cross_val_score\n",
       ">>> from sklearn.tree import DecisionTreeClassifier\n",
       ">>> clf = DecisionTreeClassifier(random_state=0)\n",
       ">>> iris = load_iris()\n",
       ">>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
       "...                             # doctest: +SKIP\n",
       "...\n",
       "array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
       "        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\tree\\_classes.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     ExtraTreeClassifier\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DecisionTreeClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1582d53-cdae-4451-8f20-aef71e78c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146bd892-24ae-4622-9641-d1b13a2dfcd0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_smoothing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-09\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Gaussian Naive Bayes (GaussianNB).\n",
       "\n",
       "Can perform online updates to model parameters via :meth:`partial_fit`.\n",
       "For details on algorithm used to update feature means and variance online,\n",
       "see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
       "\n",
       "    http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
       "\n",
       "Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "priors : array-like of shape (n_classes,)\n",
       "    Prior probabilities of the classes. If specified, the priors are not\n",
       "    adjusted according to the data.\n",
       "\n",
       "var_smoothing : float, default=1e-9\n",
       "    Portion of the largest variance of all features that is added to\n",
       "    variances for calculation stability.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "class_count_ : ndarray of shape (n_classes,)\n",
       "    number of training samples observed in each class.\n",
       "\n",
       "class_prior_ : ndarray of shape (n_classes,)\n",
       "    probability of each class.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    class labels known to the classifier.\n",
       "\n",
       "epsilon_ : float\n",
       "    absolute additive value to variances.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "sigma_ : ndarray of shape (n_classes, n_features)\n",
       "    Variance of each feature per class.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "       `sigma_` is deprecated in 1.0 and will be removed in 1.2.\n",
       "       Use `var_` instead.\n",
       "\n",
       "var_ : ndarray of shape (n_classes, n_features)\n",
       "    Variance of each feature per class.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "theta_ : ndarray of shape (n_classes, n_features)\n",
       "    mean of each feature per class.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
       "CategoricalNB : Naive Bayes classifier for categorical features.\n",
       "ComplementNB : Complement Naive Bayes classifier.\n",
       "MultinomialNB : Naive Bayes classifier for multinomial models.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
       ">>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
       ">>> from sklearn.naive_bayes import GaussianNB\n",
       ">>> clf = GaussianNB()\n",
       ">>> clf.fit(X, Y)\n",
       "GaussianNB()\n",
       ">>> print(clf.predict([[-0.8, -1]]))\n",
       "[1]\n",
       ">>> clf_pf = GaussianNB()\n",
       ">>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
       "GaussianNB()\n",
       ">>> print(clf_pf.predict([[-0.8, -1]]))\n",
       "[1]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\naive_bayes.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GaussianNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c484999c-c21b-4090-b6db-238c1d401333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Naive Bayes classifier for multivariate Bernoulli models.\n",
       "\n",
       "Like MultinomialNB, this classifier is suitable for discrete data. The\n",
       "difference is that while MultinomialNB works with occurrence counts,\n",
       "BernoulliNB is designed for binary/boolean features.\n",
       "\n",
       "Read more in the :ref:`User Guide <bernoulli_naive_bayes>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : float, default=1.0\n",
       "    Additive (Laplace/Lidstone) smoothing parameter\n",
       "    (0 for no smoothing).\n",
       "\n",
       "binarize : float or None, default=0.0\n",
       "    Threshold for binarizing (mapping to booleans) of sample features.\n",
       "    If None, input is presumed to already consist of binary vectors.\n",
       "\n",
       "fit_prior : bool, default=True\n",
       "    Whether to learn class prior probabilities or not.\n",
       "    If false, a uniform prior will be used.\n",
       "\n",
       "class_prior : array-like of shape (n_classes,), default=None\n",
       "    Prior probabilities of the classes. If specified, the priors are not\n",
       "    adjusted according to the data.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "class_count_ : ndarray of shape (n_classes,)\n",
       "    Number of samples encountered for each class during fitting. This\n",
       "    value is weighted by the sample weight when provided.\n",
       "\n",
       "class_log_prior_ : ndarray of shape (n_classes,)\n",
       "    Log probability of each class (smoothed).\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    Class labels known to the classifier\n",
       "\n",
       "feature_count_ : ndarray of shape (n_classes, n_features)\n",
       "    Number of samples encountered for each (class, feature)\n",
       "    during fitting. This value is weighted by the sample weight when\n",
       "    provided.\n",
       "\n",
       "feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
       "    Empirical log probability of features given a class, P(x_i|y).\n",
       "\n",
       "n_features_ : int\n",
       "    Number of features of each sample.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Attribute `n_features_` was deprecated in version 1.0 and will be\n",
       "        removed in 1.2. Use `n_features_in_` instead.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "CategoricalNB : Naive Bayes classifier for categorical features.\n",
       "ComplementNB : The Complement Naive Bayes classifier\n",
       "    described in Rennie et al. (2003).\n",
       "GaussianNB : Gaussian Naive Bayes (GaussianNB).\n",
       "MultinomialNB : Naive Bayes classifier for multinomial models.\n",
       "\n",
       "References\n",
       "----------\n",
       "C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
       "Information Retrieval. Cambridge University Press, pp. 234-265.\n",
       "https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n",
       "\n",
       "A. McCallum and K. Nigam (1998). A comparison of event models for naive\n",
       "Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\n",
       "Text Categorization, pp. 41-48.\n",
       "\n",
       "V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\n",
       "naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> rng = np.random.RandomState(1)\n",
       ">>> X = rng.randint(5, size=(6, 100))\n",
       ">>> Y = np.array([1, 2, 3, 4, 4, 5])\n",
       ">>> from sklearn.naive_bayes import BernoulliNB\n",
       ">>> clf = BernoulliNB()\n",
       ">>> clf.fit(X, Y)\n",
       "BernoulliNB()\n",
       ">>> print(clf.predict(X[2:3]))\n",
       "[3]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\naive_bayes.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BernoulliNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba4777c-928b-45cb-a2b2-948870b1f2b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Naive Bayes classifier for multinomial models.\n",
       "\n",
       "The multinomial Naive Bayes classifier is suitable for classification with\n",
       "discrete features (e.g., word counts for text classification). The\n",
       "multinomial distribution normally requires integer feature counts. However,\n",
       "in practice, fractional counts such as tf-idf may also work.\n",
       "\n",
       "Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : float, default=1.0\n",
       "    Additive (Laplace/Lidstone) smoothing parameter\n",
       "    (0 for no smoothing).\n",
       "\n",
       "fit_prior : bool, default=True\n",
       "    Whether to learn class prior probabilities or not.\n",
       "    If false, a uniform prior will be used.\n",
       "\n",
       "class_prior : array-like of shape (n_classes,), default=None\n",
       "    Prior probabilities of the classes. If specified, the priors are not\n",
       "    adjusted according to the data.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "class_count_ : ndarray of shape (n_classes,)\n",
       "    Number of samples encountered for each class during fitting. This\n",
       "    value is weighted by the sample weight when provided.\n",
       "\n",
       "class_log_prior_ : ndarray of shape (n_classes,)\n",
       "    Smoothed empirical log probability for each class.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    Class labels known to the classifier\n",
       "\n",
       "feature_count_ : ndarray of shape (n_classes, n_features)\n",
       "    Number of samples encountered for each (class, feature)\n",
       "    during fitting. This value is weighted by the sample weight when\n",
       "    provided.\n",
       "\n",
       "feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
       "    Empirical log probability of features\n",
       "    given a class, ``P(x_i|y)``.\n",
       "\n",
       "n_features_ : int\n",
       "    Number of features of each sample.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Attribute `n_features_` was deprecated in version 1.0 and will be\n",
       "        removed in 1.2. Use `n_features_in_` instead.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
       "CategoricalNB : Naive Bayes classifier for categorical features.\n",
       "ComplementNB : Complement Naive Bayes classifier.\n",
       "GaussianNB : Gaussian Naive Bayes.\n",
       "\n",
       "References\n",
       "----------\n",
       "C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
       "Information Retrieval. Cambridge University Press, pp. 234-265.\n",
       "https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> rng = np.random.RandomState(1)\n",
       ">>> X = rng.randint(5, size=(6, 100))\n",
       ">>> y = np.array([1, 2, 3, 4, 5, 6])\n",
       ">>> from sklearn.naive_bayes import MultinomialNB\n",
       ">>> clf = MultinomialNB()\n",
       ">>> clf.fit(X, y)\n",
       "MultinomialNB()\n",
       ">>> print(clf.predict(X[2:3]))\n",
       "[3]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\naive_bayes.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MultinomialNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ef0011-0cc2-45f4-bf80-642a98feb96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f63835-b43e-4099-9ee4-4ccd7b7fdf48",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mleaf_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'minkowski'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetric_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Classifier implementing the k-nearest neighbors vote.\n",
       "\n",
       "Read more in the :ref:`User Guide <classification>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_neighbors : int, default=5\n",
       "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
       "\n",
       "weights : {'uniform', 'distance'} or callable, default='uniform'\n",
       "    Weight function used in prediction.  Possible values:\n",
       "\n",
       "    - 'uniform' : uniform weights.  All points in each neighborhood\n",
       "      are weighted equally.\n",
       "    - 'distance' : weight points by the inverse of their distance.\n",
       "      in this case, closer neighbors of a query point will have a\n",
       "      greater influence than neighbors which are further away.\n",
       "    - [callable] : a user-defined function which accepts an\n",
       "      array of distances, and returns an array of the same shape\n",
       "      containing the weights.\n",
       "\n",
       "algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
       "    Algorithm used to compute the nearest neighbors:\n",
       "\n",
       "    - 'ball_tree' will use :class:`BallTree`\n",
       "    - 'kd_tree' will use :class:`KDTree`\n",
       "    - 'brute' will use a brute-force search.\n",
       "    - 'auto' will attempt to decide the most appropriate algorithm\n",
       "      based on the values passed to :meth:`fit` method.\n",
       "\n",
       "    Note: fitting on sparse input will override the setting of\n",
       "    this parameter, using brute force.\n",
       "\n",
       "leaf_size : int, default=30\n",
       "    Leaf size passed to BallTree or KDTree.  This can affect the\n",
       "    speed of the construction and query, as well as the memory\n",
       "    required to store the tree.  The optimal value depends on the\n",
       "    nature of the problem.\n",
       "\n",
       "p : int, default=2\n",
       "    Power parameter for the Minkowski metric. When p = 1, this is\n",
       "    equivalent to using manhattan_distance (l1), and euclidean_distance\n",
       "    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
       "\n",
       "metric : str or callable, default='minkowski'\n",
       "    The distance metric to use for the tree.  The default metric is\n",
       "    minkowski, and with p=2 is equivalent to the standard Euclidean\n",
       "    metric. For a list of available metrics, see the documentation of\n",
       "    :class:`~sklearn.metrics.DistanceMetric` and the metrics listed in\n",
       "    `sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Note that the\n",
       "    \"cosine\" metric uses :func:`~sklearn.metrics.pairwise.cosine_distances`.\n",
       "    If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
       "    must be square during fit. X may be a :term:`sparse graph`,\n",
       "    in which case only \"nonzero\" elements may be considered neighbors.\n",
       "\n",
       "metric_params : dict, default=None\n",
       "    Additional keyword arguments for the metric function.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of parallel jobs to run for neighbors search.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "    Doesn't affect :meth:`fit` method.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "classes_ : array of shape (n_classes,)\n",
       "    Class labels known to the classifier\n",
       "\n",
       "effective_metric_ : str or callble\n",
       "    The distance metric used. It will be same as the `metric` parameter\n",
       "    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
       "    'minkowski' and `p` parameter set to 2.\n",
       "\n",
       "effective_metric_params_ : dict\n",
       "    Additional keyword arguments for the metric function. For most metrics\n",
       "    will be same with `metric_params` parameter, but may also contain the\n",
       "    `p` parameter value if the `effective_metric_` attribute is set to\n",
       "    'minkowski'.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_samples_fit_ : int\n",
       "    Number of samples in the fitted data.\n",
       "\n",
       "outputs_2d_ : bool\n",
       "    False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
       "    otherwise True.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\n",
       "KNeighborsRegressor: Regression based on k-nearest neighbors.\n",
       "RadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\n",
       "NearestNeighbors: Unsupervised learner for implementing neighbor searches.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
       "for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
       "\n",
       ".. warning::\n",
       "\n",
       "   Regarding the Nearest Neighbors algorithms, if it is found that two\n",
       "   neighbors, neighbor `k+1` and `k`, have identical distances\n",
       "   but different labels, the results will depend on the ordering of the\n",
       "   training data.\n",
       "\n",
       "https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> X = [[0], [1], [2], [3]]\n",
       ">>> y = [0, 0, 1, 1]\n",
       ">>> from sklearn.neighbors import KNeighborsClassifier\n",
       ">>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
       ">>> neigh.fit(X, y)\n",
       "KNeighborsClassifier(...)\n",
       ">>> print(neigh.predict([[1.1]]))\n",
       "[0]\n",
       ">>> print(neigh.predict_proba([[0.9]]))\n",
       "[[0.666... 0.333...]]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "KNeighborsClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e24c2614-d294-425e-bf79-99d8be87f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f809441f-8bc5-41b0-b44a-b437cd7f665b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "DistanceMetric class\n",
       "\n",
       "This class provides a uniform interface to fast distance metric\n",
       "functions.  The various metrics can be accessed via the :meth:`get_metric`\n",
       "class method and the metric string identifier (see below).\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import DistanceMetric\n",
       ">>> dist = DistanceMetric.get_metric('euclidean')\n",
       ">>> X = [[0, 1, 2],\n",
       "         [3, 4, 5]]\n",
       ">>> dist.pairwise(X)\n",
       "array([[ 0.        ,  5.19615242],\n",
       "       [ 5.19615242,  0.        ]])\n",
       "\n",
       "Available Metrics\n",
       "\n",
       "The following lists the string metric identifiers and the associated\n",
       "distance metric classes:\n",
       "\n",
       "**Metrics intended for real-valued vector spaces:**\n",
       "\n",
       "==============  ====================  ========  ===============================\n",
       "identifier      class name            args      distance function\n",
       "--------------  --------------------  --------  -------------------------------\n",
       "\"euclidean\"     EuclideanDistance     -         ``sqrt(sum((x - y)^2))``\n",
       "\"manhattan\"     ManhattanDistance     -         ``sum(|x - y|)``\n",
       "\"chebyshev\"     ChebyshevDistance     -         ``max(|x - y|)``\n",
       "\"minkowski\"     MinkowskiDistance     p, w      ``sum(w * |x - y|^p)^(1/p)``\n",
       "\"wminkowski\"    WMinkowskiDistance    p, w      ``sum(|w * (x - y)|^p)^(1/p)``\n",
       "\"seuclidean\"    SEuclideanDistance    V         ``sqrt(sum((x - y)^2 / V))``\n",
       "\"mahalanobis\"   MahalanobisDistance   V or VI   ``sqrt((x - y)' V^-1 (x - y))``\n",
       "==============  ====================  ========  ===============================\n",
       "\n",
       ".. deprecated:: 1.1\n",
       "    `WMinkowskiDistance` is deprecated in version 1.1 and will be removed in version 1.3.\n",
       "    Use `MinkowskiDistance` instead. Note that in `MinkowskiDistance`, the weights are\n",
       "    applied to the absolute differences already raised to the p power. This is different from\n",
       "    `WMinkowskiDistance` where weights are applied to the absolute differences before raising\n",
       "    to the p power. The deprecation aims to remain consistent with SciPy 1.8 convention.\n",
       "\n",
       "**Metrics intended for two-dimensional vector spaces:**  Note that the haversine\n",
       "distance metric requires data in the form of [latitude, longitude] and both\n",
       "inputs and outputs are in units of radians.\n",
       "\n",
       "============  ==================  ===============================================================\n",
       "identifier    class name          distance function\n",
       "------------  ------------------  ---------------------------------------------------------------\n",
       "\"haversine\"   HaversineDistance   ``2 arcsin(sqrt(sin^2(0.5*dx) + cos(x1)cos(x2)sin^2(0.5*dy)))``\n",
       "============  ==================  ===============================================================\n",
       "\n",
       "\n",
       "**Metrics intended for integer-valued vector spaces:**  Though intended\n",
       "for integer-valued vectors, these are also valid metrics in the case of\n",
       "real-valued vectors.\n",
       "\n",
       "=============  ====================  ========================================\n",
       "identifier     class name            distance function\n",
       "-------------  --------------------  ----------------------------------------\n",
       "\"hamming\"      HammingDistance       ``N_unequal(x, y) / N_tot``\n",
       "\"canberra\"     CanberraDistance      ``sum(|x - y| / (|x| + |y|))``\n",
       "\"braycurtis\"   BrayCurtisDistance    ``sum(|x - y|) / (sum(|x|) + sum(|y|))``\n",
       "=============  ====================  ========================================\n",
       "\n",
       "**Metrics intended for boolean-valued vector spaces:**  Any nonzero entry\n",
       "is evaluated to \"True\".  In the listings below, the following\n",
       "abbreviations are used:\n",
       "\n",
       " - N  : number of dimensions\n",
       " - NTT : number of dims in which both values are True\n",
       " - NTF : number of dims in which the first value is True, second is False\n",
       " - NFT : number of dims in which the first value is False, second is True\n",
       " - NFF : number of dims in which both values are False\n",
       " - NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT\n",
       " - NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT\n",
       "\n",
       "=================  =======================  ===============================\n",
       "identifier         class name               distance function\n",
       "-----------------  -----------------------  -------------------------------\n",
       "\"jaccard\"          JaccardDistance          NNEQ / NNZ\n",
       "\"matching\"         MatchingDistance         NNEQ / N\n",
       "\"dice\"             DiceDistance             NNEQ / (NTT + NNZ)\n",
       "\"kulsinski\"        KulsinskiDistance        (NNEQ + N - NTT) / (NNEQ + N)\n",
       "\"rogerstanimoto\"   RogersTanimotoDistance   2 * NNEQ / (N + NNEQ)\n",
       "\"russellrao\"       RussellRaoDistance       (N - NTT) / N\n",
       "\"sokalmichener\"    SokalMichenerDistance    2 * NNEQ / (N + NNEQ)\n",
       "\"sokalsneath\"      SokalSneathDistance      NNEQ / (NNEQ + 0.5 * NTT)\n",
       "=================  =======================  ===============================\n",
       "\n",
       "**User-defined distance:**\n",
       "\n",
       "===========    ===============    =======\n",
       "identifier     class name         args\n",
       "-----------    ---------------    -------\n",
       "\"pyfunc\"       PyFuncDistance     func\n",
       "===========    ===============    =======\n",
       "\n",
       "Here ``func`` is a function which takes two one-dimensional numpy\n",
       "arrays, and returns a distance.  Note that in order to be used within\n",
       "the BallTree, the distance must be a true metric:\n",
       "i.e. it must satisfy the following properties\n",
       "\n",
       "1) Non-negativity: d(x, y) >= 0\n",
       "2) Identity: d(x, y) = 0 if and only if x == y\n",
       "3) Symmetry: d(x, y) = d(y, x)\n",
       "4) Triangle Inequality: d(x, y) + d(y, z) >= d(x, z)\n",
       "\n",
       "Because of the Python object overhead involved in calling the python\n",
       "function, this will be fairly slow, but it will have the same\n",
       "scaling as other distances.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\metrics\\_dist_metrics.cp38-win_amd64.pyd\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     EuclideanDistance, SEuclideanDistance, ManhattanDistance, ChebyshevDistance, MinkowskiDistance, WMinkowskiDistance, MahalanobisDistance, HammingDistance, CanberraDistance, BrayCurtisDistance, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DistanceMetric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa547f5-4426-4d8e-9b63-7e68133dfe21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rbf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'scale'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mshrinking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprobability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdecision_function_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbreak_ties\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "C-Support Vector Classification.\n",
       "\n",
       "The implementation is based on libsvm. The fit time scales at least\n",
       "quadratically with the number of samples and may be impractical\n",
       "beyond tens of thousands of samples. For large datasets\n",
       "consider using :class:`~sklearn.svm.LinearSVC` or\n",
       ":class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n",
       ":class:`~sklearn.kernel_approximation.Nystroem` transformer.\n",
       "\n",
       "The multiclass support is handled according to a one-vs-one scheme.\n",
       "\n",
       "For details on the precise mathematical formulation of the provided\n",
       "kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
       "other, see the corresponding section in the narrative documentation:\n",
       ":ref:`svm_kernels`.\n",
       "\n",
       "Read more in the :ref:`User Guide <svm_classification>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "C : float, default=1.0\n",
       "    Regularization parameter. The strength of the regularization is\n",
       "    inversely proportional to C. Must be strictly positive. The penalty\n",
       "    is a squared l2 penalty.\n",
       "\n",
       "kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\n",
       "    Specifies the kernel type to be used in the algorithm.\n",
       "    If none is given, 'rbf' will be used. If a callable is given it is\n",
       "    used to pre-compute the kernel matrix from data matrices; that matrix\n",
       "    should be an array of shape ``(n_samples, n_samples)``.\n",
       "\n",
       "degree : int, default=3\n",
       "    Degree of the polynomial kernel function ('poly').\n",
       "    Ignored by all other kernels.\n",
       "\n",
       "gamma : {'scale', 'auto'} or float, default='scale'\n",
       "    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
       "\n",
       "    - if ``gamma='scale'`` (default) is passed then it uses\n",
       "      1 / (n_features * X.var()) as value of gamma,\n",
       "    - if 'auto', uses 1 / n_features.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
       "\n",
       "coef0 : float, default=0.0\n",
       "    Independent term in kernel function.\n",
       "    It is only significant in 'poly' and 'sigmoid'.\n",
       "\n",
       "shrinking : bool, default=True\n",
       "    Whether to use the shrinking heuristic.\n",
       "    See the :ref:`User Guide <shrinking_svm>`.\n",
       "\n",
       "probability : bool, default=False\n",
       "    Whether to enable probability estimates. This must be enabled prior\n",
       "    to calling `fit`, will slow down that method as it internally uses\n",
       "    5-fold cross-validation, and `predict_proba` may be inconsistent with\n",
       "    `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n",
       "\n",
       "tol : float, default=1e-3\n",
       "    Tolerance for stopping criterion.\n",
       "\n",
       "cache_size : float, default=200\n",
       "    Specify the size of the kernel cache (in MB).\n",
       "\n",
       "class_weight : dict or 'balanced', default=None\n",
       "    Set the parameter C of class i to class_weight[i]*C for\n",
       "    SVC. If not given, all classes are supposed to have\n",
       "    weight one.\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "verbose : bool, default=False\n",
       "    Enable verbose output. Note that this setting takes advantage of a\n",
       "    per-process runtime setting in libsvm that, if enabled, may not work\n",
       "    properly in a multithreaded context.\n",
       "\n",
       "max_iter : int, default=-1\n",
       "    Hard limit on iterations within solver, or -1 for no limit.\n",
       "\n",
       "decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n",
       "    Whether to return a one-vs-rest ('ovr') decision function of shape\n",
       "    (n_samples, n_classes) as all other classifiers, or the original\n",
       "    one-vs-one ('ovo') decision function of libsvm which has shape\n",
       "    (n_samples, n_classes * (n_classes - 1) / 2). However, note that\n",
       "    internally, one-vs-one ('ovo') is always used as a multi-class strategy\n",
       "    to train models; an ovr matrix is only constructed from the ovo matrix.\n",
       "    The parameter is ignored for binary classification.\n",
       "\n",
       "    .. versionchanged:: 0.19\n",
       "        decision_function_shape is 'ovr' by default.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *decision_function_shape='ovr'* is recommended.\n",
       "\n",
       "    .. versionchanged:: 0.17\n",
       "       Deprecated *decision_function_shape='ovo' and None*.\n",
       "\n",
       "break_ties : bool, default=False\n",
       "    If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n",
       "    :term:`predict` will break ties according to the confidence values of\n",
       "    :term:`decision_function`; otherwise the first class among the tied\n",
       "    classes is returned. Please note that breaking ties comes at a\n",
       "    relatively high computational cost compared to a simple predict.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the pseudo random number generation for shuffling the data for\n",
       "    probability estimates. Ignored when `probability` is False.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "class_weight_ : ndarray of shape (n_classes,)\n",
       "    Multipliers of parameter C for each class.\n",
       "    Computed based on the ``class_weight`` parameter.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    The classes labels.\n",
       "\n",
       "coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n",
       "    Weights assigned to the features (coefficients in the primal\n",
       "    problem). This is only available in the case of a linear kernel.\n",
       "\n",
       "    `coef_` is a readonly property derived from `dual_coef_` and\n",
       "    `support_vectors_`.\n",
       "\n",
       "dual_coef_ : ndarray of shape (n_classes -1, n_SV)\n",
       "    Dual coefficients of the support vector in the decision\n",
       "    function (see :ref:`sgd_mathematical_formulation`), multiplied by\n",
       "    their targets.\n",
       "    For multiclass, coefficient for all 1-vs-1 classifiers.\n",
       "    The layout of the coefficients in the multiclass case is somewhat\n",
       "    non-trivial. See the :ref:`multi-class section of the User Guide\n",
       "    <svm_multi_class>` for details.\n",
       "\n",
       "fit_status_ : int\n",
       "    0 if correctly fitted, 1 otherwise (will raise warning)\n",
       "\n",
       "intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
       "    Constants in decision function.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)\n",
       "    Number of iterations run by the optimization routine to fit the model.\n",
       "    The shape of this attribute depends on the number of models optimized\n",
       "    which in turn depends on the number of classes.\n",
       "\n",
       "    .. versionadded:: 1.1\n",
       "\n",
       "support_ : ndarray of shape (n_SV)\n",
       "    Indices of support vectors.\n",
       "\n",
       "support_vectors_ : ndarray of shape (n_SV, n_features)\n",
       "    Support vectors.\n",
       "\n",
       "n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
       "    Number of support vectors for each class.\n",
       "\n",
       "probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
       "probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
       "    If `probability=True`, it corresponds to the parameters learned in\n",
       "    Platt scaling to produce probability estimates from decision values.\n",
       "    If `probability=False`, it's an empty array. Platt scaling uses the\n",
       "    logistic function\n",
       "    ``1 / (1 + exp(decision_value * probA_ + probB_))``\n",
       "    where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n",
       "    more information on the multiclass case and training procedure see\n",
       "    section 8 of [1]_.\n",
       "\n",
       "shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
       "    Array dimensions of training vector ``X``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "SVR : Support Vector Machine for Regression implemented using libsvm.\n",
       "\n",
       "LinearSVC : Scalable Linear Support Vector Machine for classification\n",
       "    implemented using liblinear. Check the See Also section of\n",
       "    LinearSVC for more comparison element.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] `LIBSVM: A Library for Support Vector Machines\n",
       "    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
       "\n",
       ".. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n",
       "    machines and comparison to regularizedlikelihood methods.\"\n",
       "    <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.pipeline import make_pipeline\n",
       ">>> from sklearn.preprocessing import StandardScaler\n",
       ">>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
       ">>> y = np.array([1, 1, 2, 2])\n",
       ">>> from sklearn.svm import SVC\n",
       ">>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
       ">>> clf.fit(X, y)\n",
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])\n",
       "\n",
       ">>> print(clf.predict([[-0.8, -1]]))\n",
       "[1]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "SVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4277d341-87ab-4ec9-9f3b-1f86b2ddab21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'squared_hinge'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdual\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mintercept_scaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Linear Support Vector Classification.\n",
       "\n",
       "Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
       "liblinear rather than libsvm, so it has more flexibility in the choice of\n",
       "penalties and loss functions and should scale better to large numbers of\n",
       "samples.\n",
       "\n",
       "This class supports both dense and sparse input and the multiclass support\n",
       "is handled according to a one-vs-the-rest scheme.\n",
       "\n",
       "Read more in the :ref:`User Guide <svm_classification>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "penalty : {'l1', 'l2'}, default='l2'\n",
       "    Specifies the norm used in the penalization. The 'l2'\n",
       "    penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
       "    vectors that are sparse.\n",
       "\n",
       "loss : {'hinge', 'squared_hinge'}, default='squared_hinge'\n",
       "    Specifies the loss function. 'hinge' is the standard SVM loss\n",
       "    (used e.g. by the SVC class) while 'squared_hinge' is the\n",
       "    square of the hinge loss. The combination of ``penalty='l1'``\n",
       "    and ``loss='hinge'`` is not supported.\n",
       "\n",
       "dual : bool, default=True\n",
       "    Select the algorithm to either solve the dual or primal\n",
       "    optimization problem. Prefer dual=False when n_samples > n_features.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for stopping criteria.\n",
       "\n",
       "C : float, default=1.0\n",
       "    Regularization parameter. The strength of the regularization is\n",
       "    inversely proportional to C. Must be strictly positive.\n",
       "\n",
       "multi_class : {'ovr', 'crammer_singer'}, default='ovr'\n",
       "    Determines the multi-class strategy if `y` contains more than\n",
       "    two classes.\n",
       "    ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
       "    ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
       "    While `crammer_singer` is interesting from a theoretical perspective\n",
       "    as it is consistent, it is seldom used in practice as it rarely leads\n",
       "    to better accuracy and is more expensive to compute.\n",
       "    If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
       "    will be ignored.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Whether to calculate the intercept for this model. If set\n",
       "    to false, no intercept will be used in calculations\n",
       "    (i.e. data is expected to be already centered).\n",
       "\n",
       "intercept_scaling : float, default=1\n",
       "    When self.fit_intercept is True, instance vector x becomes\n",
       "    ``[x, self.intercept_scaling]``,\n",
       "    i.e. a \"synthetic\" feature with constant value equals to\n",
       "    intercept_scaling is appended to the instance vector.\n",
       "    The intercept becomes intercept_scaling * synthetic feature weight\n",
       "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
       "    as all other features.\n",
       "    To lessen the effect of regularization on synthetic feature weight\n",
       "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
       "\n",
       "class_weight : dict or 'balanced', default=None\n",
       "    Set the parameter C of class i to ``class_weight[i]*C`` for\n",
       "    SVC. If not given, all classes are supposed to have\n",
       "    weight one.\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Enable verbose output. Note that this setting takes advantage of a\n",
       "    per-process runtime setting in liblinear that, if enabled, may not work\n",
       "    properly in a multithreaded context.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the pseudo random number generation for shuffling the data for\n",
       "    the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\n",
       "    underlying implementation of :class:`LinearSVC` is not random and\n",
       "    ``random_state`` has no effect on the results.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "max_iter : int, default=1000\n",
       "    The maximum number of iterations to be run.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\n",
       "    Weights assigned to the features (coefficients in the primal\n",
       "    problem).\n",
       "\n",
       "    ``coef_`` is a readonly property derived from ``raw_coef_`` that\n",
       "    follows the internal memory layout of liblinear.\n",
       "\n",
       "intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
       "    Constants in decision function.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    The unique classes labels.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_iter_ : int\n",
       "    Maximum number of iterations run across all classes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "SVC : Implementation of Support Vector Machine classifier using libsvm:\n",
       "    the kernel can be non-linear but its SMO algorithm does not\n",
       "    scale to large number of samples as LinearSVC does.\n",
       "\n",
       "    Furthermore SVC multi-class mode is implemented using one\n",
       "    vs one scheme while LinearSVC uses one vs the rest. It is\n",
       "    possible to implement one vs the rest with SVC by using the\n",
       "    :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
       "\n",
       "    Finally SVC can fit dense data without memory copy if the input\n",
       "    is C-contiguous. Sparse data will still incur memory copy though.\n",
       "\n",
       "sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same\n",
       "    cost function as LinearSVC\n",
       "    by adjusting the penalty and loss parameters. In addition it requires\n",
       "    less memory, allows incremental (online) learning, and implements\n",
       "    various loss functions and regularization regimes.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The underlying C implementation uses a random number generator to\n",
       "select features when fitting the model. It is thus not uncommon\n",
       "to have slightly different results for the same input data. If\n",
       "that happens, try with a smaller ``tol`` parameter.\n",
       "\n",
       "The underlying implementation, liblinear, uses a sparse internal\n",
       "representation for the data that will incur a memory copy.\n",
       "\n",
       "Predict output may not match that of standalone liblinear in certain\n",
       "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
       "in the narrative documentation.\n",
       "\n",
       "References\n",
       "----------\n",
       "`LIBLINEAR: A Library for Large Linear Classification\n",
       "<https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.svm import LinearSVC\n",
       ">>> from sklearn.pipeline import make_pipeline\n",
       ">>> from sklearn.preprocessing import StandardScaler\n",
       ">>> from sklearn.datasets import make_classification\n",
       ">>> X, y = make_classification(n_features=4, random_state=0)\n",
       ">>> clf = make_pipeline(StandardScaler(),\n",
       "...                     LinearSVC(random_state=0, tol=1e-5))\n",
       ">>> clf.fit(X, y)\n",
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n",
       "\n",
       ">>> print(clf.named_steps['linearsvc'].coef_)\n",
       "[[0.141...   0.526... 0.679... 0.493...]]\n",
       "\n",
       ">>> print(clf.named_steps['linearsvc'].intercept_)\n",
       "[0.1693...]\n",
       ">>> print(clf.predict([[0, 0, 0, 0]]))\n",
       "[1]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LinearSVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6f52f-19b5-4855-b0af-242712799c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
