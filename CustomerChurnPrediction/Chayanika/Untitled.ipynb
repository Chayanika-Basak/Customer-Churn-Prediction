{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083df0b1-3535-4691-8b8a-74d006389c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f5a9b39-0ca0-4366-b604-a272eac0d54f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gini'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msplitter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'best'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "A decision tree classifier.\n",
       "\n",
       "Read more in the :ref:`User Guide <tree>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
       "    The function to measure the quality of a split. Supported criteria are\n",
       "    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
       "    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
       "\n",
       "splitter : {\"best\", \"random\"}, default=\"best\"\n",
       "    The strategy used to choose the split at each node. Supported\n",
       "    strategies are \"best\" to choose the best split and \"random\" to choose\n",
       "    the best random split.\n",
       "\n",
       "max_depth : int, default=None\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "        - If int, then consider `max_features` features at each split.\n",
       "        - If float, then `max_features` is a fraction and\n",
       "          `int(max_features * n_features)` features are considered at each\n",
       "          split.\n",
       "        - If \"auto\", then `max_features=sqrt(n_features)`.\n",
       "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "        - If \"log2\", then `max_features=log2(n_features)`.\n",
       "        - If None, then `max_features=n_features`.\n",
       "\n",
       "        .. deprecated:: 1.1\n",
       "            The `\"auto\"` option was deprecated in 1.1 and will be removed\n",
       "            in 1.3.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the randomness of the estimator. The features are always\n",
       "    randomly permuted at each split, even if ``splitter`` is set to\n",
       "    ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
       "    select ``max_features`` at random at each split before finding the best\n",
       "    split among them. But the best found split may vary across different\n",
       "    runs, even if ``max_features=n_features``. That is the case, if the\n",
       "    improvement of the criterion is identical for several splits and one\n",
       "    split has to be selected at random. To obtain a deterministic behaviour\n",
       "    during fitting, ``random_state`` has to be fixed to an integer.\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "class_weight : dict, list of dict or \"balanced\", default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If None, all classes are supposed to have weight one. For\n",
       "    multi-output problems, a list of dicts can be provided in the same\n",
       "    order as the columns of y.\n",
       "\n",
       "    Note that for multioutput (including multilabel) weights should be\n",
       "    defined for each class of every column in its own dict. For example,\n",
       "    for four-class multilabel classification weights should be\n",
       "    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
       "    [{1:1}, {2:5}, {3:1}, {4:1}].\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``\n",
       "\n",
       "    For multi-output, the weights of each column of y will be multiplied.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
       "    The classes labels (single output problem),\n",
       "    or a list of arrays of class labels (multi-output problem).\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance [4]_.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "max_features_ : int\n",
       "    The inferred value of max_features.\n",
       "\n",
       "n_classes_ : int or list of int\n",
       "    The number of classes (for single output problems),\n",
       "    or a list containing the number of classes for each\n",
       "    output (for multi-output problems).\n",
       "\n",
       "n_features_ : int\n",
       "    The number of features when ``fit`` is performed.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "       `n_features_` is deprecated in 1.0 and will be removed in\n",
       "       1.2. Use `n_features_in_` instead.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs when ``fit`` is performed.\n",
       "\n",
       "tree_ : Tree instance\n",
       "    The underlying Tree object. Please refer to\n",
       "    ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
       "    :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
       "    for basic usage of these attributes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DecisionTreeRegressor : A decision tree regressor.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
       "function on the outputs of :meth:`predict_proba`. This means that in\n",
       "case the highest predicted probabilities are tied, the classifier will\n",
       "predict the tied class with the lowest index in :term:`classes_`.\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       ".. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
       "\n",
       ".. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
       "       and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
       "\n",
       ".. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
       "       Learning\", Springer, 2009.\n",
       "\n",
       ".. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
       "       https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_iris\n",
       ">>> from sklearn.model_selection import cross_val_score\n",
       ">>> from sklearn.tree import DecisionTreeClassifier\n",
       ">>> clf = DecisionTreeClassifier(random_state=0)\n",
       ">>> iris = load_iris()\n",
       ">>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
       "...                             # doctest: +SKIP\n",
       "...\n",
       "array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
       "        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\tree\\_classes.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     ExtraTreeClassifier\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DecisionTreeClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1582d53-cdae-4451-8f20-aef71e78c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146bd892-24ae-4622-9641-d1b13a2dfcd0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_smoothing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-09\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Gaussian Naive Bayes (GaussianNB).\n",
       "\n",
       "Can perform online updates to model parameters via :meth:`partial_fit`.\n",
       "For details on algorithm used to update feature means and variance online,\n",
       "see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
       "\n",
       "    http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
       "\n",
       "Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "priors : array-like of shape (n_classes,)\n",
       "    Prior probabilities of the classes. If specified, the priors are not\n",
       "    adjusted according to the data.\n",
       "\n",
       "var_smoothing : float, default=1e-9\n",
       "    Portion of the largest variance of all features that is added to\n",
       "    variances for calculation stability.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "class_count_ : ndarray of shape (n_classes,)\n",
       "    number of training samples observed in each class.\n",
       "\n",
       "class_prior_ : ndarray of shape (n_classes,)\n",
       "    probability of each class.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    class labels known to the classifier.\n",
       "\n",
       "epsilon_ : float\n",
       "    absolute additive value to variances.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "sigma_ : ndarray of shape (n_classes, n_features)\n",
       "    Variance of each feature per class.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "       `sigma_` is deprecated in 1.0 and will be removed in 1.2.\n",
       "       Use `var_` instead.\n",
       "\n",
       "var_ : ndarray of shape (n_classes, n_features)\n",
       "    Variance of each feature per class.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "theta_ : ndarray of shape (n_classes, n_features)\n",
       "    mean of each feature per class.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
       "CategoricalNB : Naive Bayes classifier for categorical features.\n",
       "ComplementNB : Complement Naive Bayes classifier.\n",
       "MultinomialNB : Naive Bayes classifier for multinomial models.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
       ">>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
       ">>> from sklearn.naive_bayes import GaussianNB\n",
       ">>> clf = GaussianNB()\n",
       ">>> clf.fit(X, Y)\n",
       "GaussianNB()\n",
       ">>> print(clf.predict([[-0.8, -1]]))\n",
       "[1]\n",
       ">>> clf_pf = GaussianNB()\n",
       ">>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
       "GaussianNB()\n",
       ">>> print(clf_pf.predict([[-0.8, -1]]))\n",
       "[1]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\naive_bayes.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GaussianNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c484999c-c21b-4090-b6db-238c1d401333",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Naive Bayes classifier for multivariate Bernoulli models.\n",
       "\n",
       "Like MultinomialNB, this classifier is suitable for discrete data. The\n",
       "difference is that while MultinomialNB works with occurrence counts,\n",
       "BernoulliNB is designed for binary/boolean features.\n",
       "\n",
       "Read more in the :ref:`User Guide <bernoulli_naive_bayes>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : float, default=1.0\n",
       "    Additive (Laplace/Lidstone) smoothing parameter\n",
       "    (0 for no smoothing).\n",
       "\n",
       "binarize : float or None, default=0.0\n",
       "    Threshold for binarizing (mapping to booleans) of sample features.\n",
       "    If None, input is presumed to already consist of binary vectors.\n",
       "\n",
       "fit_prior : bool, default=True\n",
       "    Whether to learn class prior probabilities or not.\n",
       "    If false, a uniform prior will be used.\n",
       "\n",
       "class_prior : array-like of shape (n_classes,), default=None\n",
       "    Prior probabilities of the classes. If specified, the priors are not\n",
       "    adjusted according to the data.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "class_count_ : ndarray of shape (n_classes,)\n",
       "    Number of samples encountered for each class during fitting. This\n",
       "    value is weighted by the sample weight when provided.\n",
       "\n",
       "class_log_prior_ : ndarray of shape (n_classes,)\n",
       "    Log probability of each class (smoothed).\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    Class labels known to the classifier\n",
       "\n",
       "feature_count_ : ndarray of shape (n_classes, n_features)\n",
       "    Number of samples encountered for each (class, feature)\n",
       "    during fitting. This value is weighted by the sample weight when\n",
       "    provided.\n",
       "\n",
       "feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
       "    Empirical log probability of features given a class, P(x_i|y).\n",
       "\n",
       "n_features_ : int\n",
       "    Number of features of each sample.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Attribute `n_features_` was deprecated in version 1.0 and will be\n",
       "        removed in 1.2. Use `n_features_in_` instead.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "CategoricalNB : Naive Bayes classifier for categorical features.\n",
       "ComplementNB : The Complement Naive Bayes classifier\n",
       "    described in Rennie et al. (2003).\n",
       "GaussianNB : Gaussian Naive Bayes (GaussianNB).\n",
       "MultinomialNB : Naive Bayes classifier for multinomial models.\n",
       "\n",
       "References\n",
       "----------\n",
       "C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
       "Information Retrieval. Cambridge University Press, pp. 234-265.\n",
       "https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n",
       "\n",
       "A. McCallum and K. Nigam (1998). A comparison of event models for naive\n",
       "Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for\n",
       "Text Categorization, pp. 41-48.\n",
       "\n",
       "V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with\n",
       "naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> rng = np.random.RandomState(1)\n",
       ">>> X = rng.randint(5, size=(6, 100))\n",
       ">>> Y = np.array([1, 2, 3, 4, 4, 5])\n",
       ">>> from sklearn.naive_bayes import BernoulliNB\n",
       ">>> clf = BernoulliNB()\n",
       ">>> clf.fit(X, Y)\n",
       "BernoulliNB()\n",
       ">>> print(clf.predict(X[2:3]))\n",
       "[3]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\naive_bayes.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BernoulliNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba4777c-928b-45cb-a2b2-948870b1f2b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Naive Bayes classifier for multinomial models.\n",
       "\n",
       "The multinomial Naive Bayes classifier is suitable for classification with\n",
       "discrete features (e.g., word counts for text classification). The\n",
       "multinomial distribution normally requires integer feature counts. However,\n",
       "in practice, fractional counts such as tf-idf may also work.\n",
       "\n",
       "Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "alpha : float, default=1.0\n",
       "    Additive (Laplace/Lidstone) smoothing parameter\n",
       "    (0 for no smoothing).\n",
       "\n",
       "fit_prior : bool, default=True\n",
       "    Whether to learn class prior probabilities or not.\n",
       "    If false, a uniform prior will be used.\n",
       "\n",
       "class_prior : array-like of shape (n_classes,), default=None\n",
       "    Prior probabilities of the classes. If specified, the priors are not\n",
       "    adjusted according to the data.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "class_count_ : ndarray of shape (n_classes,)\n",
       "    Number of samples encountered for each class during fitting. This\n",
       "    value is weighted by the sample weight when provided.\n",
       "\n",
       "class_log_prior_ : ndarray of shape (n_classes,)\n",
       "    Smoothed empirical log probability for each class.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    Class labels known to the classifier\n",
       "\n",
       "feature_count_ : ndarray of shape (n_classes, n_features)\n",
       "    Number of samples encountered for each (class, feature)\n",
       "    during fitting. This value is weighted by the sample weight when\n",
       "    provided.\n",
       "\n",
       "feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
       "    Empirical log probability of features\n",
       "    given a class, ``P(x_i|y)``.\n",
       "\n",
       "n_features_ : int\n",
       "    Number of features of each sample.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Attribute `n_features_` was deprecated in version 1.0 and will be\n",
       "        removed in 1.2. Use `n_features_in_` instead.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
       "CategoricalNB : Naive Bayes classifier for categorical features.\n",
       "ComplementNB : Complement Naive Bayes classifier.\n",
       "GaussianNB : Gaussian Naive Bayes.\n",
       "\n",
       "References\n",
       "----------\n",
       "C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
       "Information Retrieval. Cambridge University Press, pp. 234-265.\n",
       "https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> rng = np.random.RandomState(1)\n",
       ">>> X = rng.randint(5, size=(6, 100))\n",
       ">>> y = np.array([1, 2, 3, 4, 5, 6])\n",
       ">>> from sklearn.naive_bayes import MultinomialNB\n",
       ">>> clf = MultinomialNB()\n",
       ">>> clf.fit(X, y)\n",
       "MultinomialNB()\n",
       ">>> print(clf.predict(X[2:3]))\n",
       "[3]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\naive_bayes.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MultinomialNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ef0011-0cc2-45f4-bf80-642a98feb96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f63835-b43e-4099-9ee4-4ccd7b7fdf48",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mleaf_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'minkowski'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetric_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Classifier implementing the k-nearest neighbors vote.\n",
       "\n",
       "Read more in the :ref:`User Guide <classification>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_neighbors : int, default=5\n",
       "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
       "\n",
       "weights : {'uniform', 'distance'} or callable, default='uniform'\n",
       "    Weight function used in prediction.  Possible values:\n",
       "\n",
       "    - 'uniform' : uniform weights.  All points in each neighborhood\n",
       "      are weighted equally.\n",
       "    - 'distance' : weight points by the inverse of their distance.\n",
       "      in this case, closer neighbors of a query point will have a\n",
       "      greater influence than neighbors which are further away.\n",
       "    - [callable] : a user-defined function which accepts an\n",
       "      array of distances, and returns an array of the same shape\n",
       "      containing the weights.\n",
       "\n",
       "algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
       "    Algorithm used to compute the nearest neighbors:\n",
       "\n",
       "    - 'ball_tree' will use :class:`BallTree`\n",
       "    - 'kd_tree' will use :class:`KDTree`\n",
       "    - 'brute' will use a brute-force search.\n",
       "    - 'auto' will attempt to decide the most appropriate algorithm\n",
       "      based on the values passed to :meth:`fit` method.\n",
       "\n",
       "    Note: fitting on sparse input will override the setting of\n",
       "    this parameter, using brute force.\n",
       "\n",
       "leaf_size : int, default=30\n",
       "    Leaf size passed to BallTree or KDTree.  This can affect the\n",
       "    speed of the construction and query, as well as the memory\n",
       "    required to store the tree.  The optimal value depends on the\n",
       "    nature of the problem.\n",
       "\n",
       "p : int, default=2\n",
       "    Power parameter for the Minkowski metric. When p = 1, this is\n",
       "    equivalent to using manhattan_distance (l1), and euclidean_distance\n",
       "    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
       "\n",
       "metric : str or callable, default='minkowski'\n",
       "    The distance metric to use for the tree.  The default metric is\n",
       "    minkowski, and with p=2 is equivalent to the standard Euclidean\n",
       "    metric. For a list of available metrics, see the documentation of\n",
       "    :class:`~sklearn.metrics.DistanceMetric` and the metrics listed in\n",
       "    `sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Note that the\n",
       "    \"cosine\" metric uses :func:`~sklearn.metrics.pairwise.cosine_distances`.\n",
       "    If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
       "    must be square during fit. X may be a :term:`sparse graph`,\n",
       "    in which case only \"nonzero\" elements may be considered neighbors.\n",
       "\n",
       "metric_params : dict, default=None\n",
       "    Additional keyword arguments for the metric function.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of parallel jobs to run for neighbors search.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "    Doesn't affect :meth:`fit` method.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "classes_ : array of shape (n_classes,)\n",
       "    Class labels known to the classifier\n",
       "\n",
       "effective_metric_ : str or callble\n",
       "    The distance metric used. It will be same as the `metric` parameter\n",
       "    or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
       "    'minkowski' and `p` parameter set to 2.\n",
       "\n",
       "effective_metric_params_ : dict\n",
       "    Additional keyword arguments for the metric function. For most metrics\n",
       "    will be same with `metric_params` parameter, but may also contain the\n",
       "    `p` parameter value if the `effective_metric_` attribute is set to\n",
       "    'minkowski'.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_samples_fit_ : int\n",
       "    Number of samples in the fitted data.\n",
       "\n",
       "outputs_2d_ : bool\n",
       "    False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
       "    otherwise True.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\n",
       "KNeighborsRegressor: Regression based on k-nearest neighbors.\n",
       "RadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\n",
       "NearestNeighbors: Unsupervised learner for implementing neighbor searches.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
       "for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
       "\n",
       ".. warning::\n",
       "\n",
       "   Regarding the Nearest Neighbors algorithms, if it is found that two\n",
       "   neighbors, neighbor `k+1` and `k`, have identical distances\n",
       "   but different labels, the results will depend on the ordering of the\n",
       "   training data.\n",
       "\n",
       "https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> X = [[0], [1], [2], [3]]\n",
       ">>> y = [0, 0, 1, 1]\n",
       ">>> from sklearn.neighbors import KNeighborsClassifier\n",
       ">>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
       ">>> neigh.fit(X, y)\n",
       "KNeighborsClassifier(...)\n",
       ">>> print(neigh.predict([[1.1]]))\n",
       "[0]\n",
       ">>> print(neigh.predict_proba([[0.9]]))\n",
       "[[0.666... 0.333...]]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "KNeighborsClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e24c2614-d294-425e-bf79-99d8be87f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f809441f-8bc5-41b0-b44a-b437cd7f665b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "DistanceMetric class\n",
       "\n",
       "This class provides a uniform interface to fast distance metric\n",
       "functions.  The various metrics can be accessed via the :meth:`get_metric`\n",
       "class method and the metric string identifier (see below).\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import DistanceMetric\n",
       ">>> dist = DistanceMetric.get_metric('euclidean')\n",
       ">>> X = [[0, 1, 2],\n",
       "         [3, 4, 5]]\n",
       ">>> dist.pairwise(X)\n",
       "array([[ 0.        ,  5.19615242],\n",
       "       [ 5.19615242,  0.        ]])\n",
       "\n",
       "Available Metrics\n",
       "\n",
       "The following lists the string metric identifiers and the associated\n",
       "distance metric classes:\n",
       "\n",
       "**Metrics intended for real-valued vector spaces:**\n",
       "\n",
       "==============  ====================  ========  ===============================\n",
       "identifier      class name            args      distance function\n",
       "--------------  --------------------  --------  -------------------------------\n",
       "\"euclidean\"     EuclideanDistance     -         ``sqrt(sum((x - y)^2))``\n",
       "\"manhattan\"     ManhattanDistance     -         ``sum(|x - y|)``\n",
       "\"chebyshev\"     ChebyshevDistance     -         ``max(|x - y|)``\n",
       "\"minkowski\"     MinkowskiDistance     p, w      ``sum(w * |x - y|^p)^(1/p)``\n",
       "\"wminkowski\"    WMinkowskiDistance    p, w      ``sum(|w * (x - y)|^p)^(1/p)``\n",
       "\"seuclidean\"    SEuclideanDistance    V         ``sqrt(sum((x - y)^2 / V))``\n",
       "\"mahalanobis\"   MahalanobisDistance   V or VI   ``sqrt((x - y)' V^-1 (x - y))``\n",
       "==============  ====================  ========  ===============================\n",
       "\n",
       ".. deprecated:: 1.1\n",
       "    `WMinkowskiDistance` is deprecated in version 1.1 and will be removed in version 1.3.\n",
       "    Use `MinkowskiDistance` instead. Note that in `MinkowskiDistance`, the weights are\n",
       "    applied to the absolute differences already raised to the p power. This is different from\n",
       "    `WMinkowskiDistance` where weights are applied to the absolute differences before raising\n",
       "    to the p power. The deprecation aims to remain consistent with SciPy 1.8 convention.\n",
       "\n",
       "**Metrics intended for two-dimensional vector spaces:**  Note that the haversine\n",
       "distance metric requires data in the form of [latitude, longitude] and both\n",
       "inputs and outputs are in units of radians.\n",
       "\n",
       "============  ==================  ===============================================================\n",
       "identifier    class name          distance function\n",
       "------------  ------------------  ---------------------------------------------------------------\n",
       "\"haversine\"   HaversineDistance   ``2 arcsin(sqrt(sin^2(0.5*dx) + cos(x1)cos(x2)sin^2(0.5*dy)))``\n",
       "============  ==================  ===============================================================\n",
       "\n",
       "\n",
       "**Metrics intended for integer-valued vector spaces:**  Though intended\n",
       "for integer-valued vectors, these are also valid metrics in the case of\n",
       "real-valued vectors.\n",
       "\n",
       "=============  ====================  ========================================\n",
       "identifier     class name            distance function\n",
       "-------------  --------------------  ----------------------------------------\n",
       "\"hamming\"      HammingDistance       ``N_unequal(x, y) / N_tot``\n",
       "\"canberra\"     CanberraDistance      ``sum(|x - y| / (|x| + |y|))``\n",
       "\"braycurtis\"   BrayCurtisDistance    ``sum(|x - y|) / (sum(|x|) + sum(|y|))``\n",
       "=============  ====================  ========================================\n",
       "\n",
       "**Metrics intended for boolean-valued vector spaces:**  Any nonzero entry\n",
       "is evaluated to \"True\".  In the listings below, the following\n",
       "abbreviations are used:\n",
       "\n",
       " - N  : number of dimensions\n",
       " - NTT : number of dims in which both values are True\n",
       " - NTF : number of dims in which the first value is True, second is False\n",
       " - NFT : number of dims in which the first value is False, second is True\n",
       " - NFF : number of dims in which both values are False\n",
       " - NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT\n",
       " - NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT\n",
       "\n",
       "=================  =======================  ===============================\n",
       "identifier         class name               distance function\n",
       "-----------------  -----------------------  -------------------------------\n",
       "\"jaccard\"          JaccardDistance          NNEQ / NNZ\n",
       "\"matching\"         MatchingDistance         NNEQ / N\n",
       "\"dice\"             DiceDistance             NNEQ / (NTT + NNZ)\n",
       "\"kulsinski\"        KulsinskiDistance        (NNEQ + N - NTT) / (NNEQ + N)\n",
       "\"rogerstanimoto\"   RogersTanimotoDistance   2 * NNEQ / (N + NNEQ)\n",
       "\"russellrao\"       RussellRaoDistance       (N - NTT) / N\n",
       "\"sokalmichener\"    SokalMichenerDistance    2 * NNEQ / (N + NNEQ)\n",
       "\"sokalsneath\"      SokalSneathDistance      NNEQ / (NNEQ + 0.5 * NTT)\n",
       "=================  =======================  ===============================\n",
       "\n",
       "**User-defined distance:**\n",
       "\n",
       "===========    ===============    =======\n",
       "identifier     class name         args\n",
       "-----------    ---------------    -------\n",
       "\"pyfunc\"       PyFuncDistance     func\n",
       "===========    ===============    =======\n",
       "\n",
       "Here ``func`` is a function which takes two one-dimensional numpy\n",
       "arrays, and returns a distance.  Note that in order to be used within\n",
       "the BallTree, the distance must be a true metric:\n",
       "i.e. it must satisfy the following properties\n",
       "\n",
       "1) Non-negativity: d(x, y) >= 0\n",
       "2) Identity: d(x, y) = 0 if and only if x == y\n",
       "3) Symmetry: d(x, y) = d(y, x)\n",
       "4) Triangle Inequality: d(x, y) + d(y, z) >= d(x, z)\n",
       "\n",
       "Because of the Python object overhead involved in calling the python\n",
       "function, this will be fairly slow, but it will have the same\n",
       "scaling as other distances.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\metrics\\_dist_metrics.cp38-win_amd64.pyd\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     EuclideanDistance, SEuclideanDistance, ManhattanDistance, ChebyshevDistance, MinkowskiDistance, WMinkowskiDistance, MahalanobisDistance, HammingDistance, CanberraDistance, BrayCurtisDistance, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DistanceMetric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa547f5-4426-4d8e-9b63-7e68133dfe21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rbf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'scale'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mshrinking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprobability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdecision_function_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbreak_ties\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "C-Support Vector Classification.\n",
       "\n",
       "The implementation is based on libsvm. The fit time scales at least\n",
       "quadratically with the number of samples and may be impractical\n",
       "beyond tens of thousands of samples. For large datasets\n",
       "consider using :class:`~sklearn.svm.LinearSVC` or\n",
       ":class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n",
       ":class:`~sklearn.kernel_approximation.Nystroem` transformer.\n",
       "\n",
       "The multiclass support is handled according to a one-vs-one scheme.\n",
       "\n",
       "For details on the precise mathematical formulation of the provided\n",
       "kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
       "other, see the corresponding section in the narrative documentation:\n",
       ":ref:`svm_kernels`.\n",
       "\n",
       "Read more in the :ref:`User Guide <svm_classification>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "C : float, default=1.0\n",
       "    Regularization parameter. The strength of the regularization is\n",
       "    inversely proportional to C. Must be strictly positive. The penalty\n",
       "    is a squared l2 penalty.\n",
       "\n",
       "kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\n",
       "    Specifies the kernel type to be used in the algorithm.\n",
       "    If none is given, 'rbf' will be used. If a callable is given it is\n",
       "    used to pre-compute the kernel matrix from data matrices; that matrix\n",
       "    should be an array of shape ``(n_samples, n_samples)``.\n",
       "\n",
       "degree : int, default=3\n",
       "    Degree of the polynomial kernel function ('poly').\n",
       "    Ignored by all other kernels.\n",
       "\n",
       "gamma : {'scale', 'auto'} or float, default='scale'\n",
       "    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
       "\n",
       "    - if ``gamma='scale'`` (default) is passed then it uses\n",
       "      1 / (n_features * X.var()) as value of gamma,\n",
       "    - if 'auto', uses 1 / n_features.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
       "\n",
       "coef0 : float, default=0.0\n",
       "    Independent term in kernel function.\n",
       "    It is only significant in 'poly' and 'sigmoid'.\n",
       "\n",
       "shrinking : bool, default=True\n",
       "    Whether to use the shrinking heuristic.\n",
       "    See the :ref:`User Guide <shrinking_svm>`.\n",
       "\n",
       "probability : bool, default=False\n",
       "    Whether to enable probability estimates. This must be enabled prior\n",
       "    to calling `fit`, will slow down that method as it internally uses\n",
       "    5-fold cross-validation, and `predict_proba` may be inconsistent with\n",
       "    `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n",
       "\n",
       "tol : float, default=1e-3\n",
       "    Tolerance for stopping criterion.\n",
       "\n",
       "cache_size : float, default=200\n",
       "    Specify the size of the kernel cache (in MB).\n",
       "\n",
       "class_weight : dict or 'balanced', default=None\n",
       "    Set the parameter C of class i to class_weight[i]*C for\n",
       "    SVC. If not given, all classes are supposed to have\n",
       "    weight one.\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "verbose : bool, default=False\n",
       "    Enable verbose output. Note that this setting takes advantage of a\n",
       "    per-process runtime setting in libsvm that, if enabled, may not work\n",
       "    properly in a multithreaded context.\n",
       "\n",
       "max_iter : int, default=-1\n",
       "    Hard limit on iterations within solver, or -1 for no limit.\n",
       "\n",
       "decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n",
       "    Whether to return a one-vs-rest ('ovr') decision function of shape\n",
       "    (n_samples, n_classes) as all other classifiers, or the original\n",
       "    one-vs-one ('ovo') decision function of libsvm which has shape\n",
       "    (n_samples, n_classes * (n_classes - 1) / 2). However, note that\n",
       "    internally, one-vs-one ('ovo') is always used as a multi-class strategy\n",
       "    to train models; an ovr matrix is only constructed from the ovo matrix.\n",
       "    The parameter is ignored for binary classification.\n",
       "\n",
       "    .. versionchanged:: 0.19\n",
       "        decision_function_shape is 'ovr' by default.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *decision_function_shape='ovr'* is recommended.\n",
       "\n",
       "    .. versionchanged:: 0.17\n",
       "       Deprecated *decision_function_shape='ovo' and None*.\n",
       "\n",
       "break_ties : bool, default=False\n",
       "    If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n",
       "    :term:`predict` will break ties according to the confidence values of\n",
       "    :term:`decision_function`; otherwise the first class among the tied\n",
       "    classes is returned. Please note that breaking ties comes at a\n",
       "    relatively high computational cost compared to a simple predict.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the pseudo random number generation for shuffling the data for\n",
       "    probability estimates. Ignored when `probability` is False.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "class_weight_ : ndarray of shape (n_classes,)\n",
       "    Multipliers of parameter C for each class.\n",
       "    Computed based on the ``class_weight`` parameter.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    The classes labels.\n",
       "\n",
       "coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n",
       "    Weights assigned to the features (coefficients in the primal\n",
       "    problem). This is only available in the case of a linear kernel.\n",
       "\n",
       "    `coef_` is a readonly property derived from `dual_coef_` and\n",
       "    `support_vectors_`.\n",
       "\n",
       "dual_coef_ : ndarray of shape (n_classes -1, n_SV)\n",
       "    Dual coefficients of the support vector in the decision\n",
       "    function (see :ref:`sgd_mathematical_formulation`), multiplied by\n",
       "    their targets.\n",
       "    For multiclass, coefficient for all 1-vs-1 classifiers.\n",
       "    The layout of the coefficients in the multiclass case is somewhat\n",
       "    non-trivial. See the :ref:`multi-class section of the User Guide\n",
       "    <svm_multi_class>` for details.\n",
       "\n",
       "fit_status_ : int\n",
       "    0 if correctly fitted, 1 otherwise (will raise warning)\n",
       "\n",
       "intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
       "    Constants in decision function.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)\n",
       "    Number of iterations run by the optimization routine to fit the model.\n",
       "    The shape of this attribute depends on the number of models optimized\n",
       "    which in turn depends on the number of classes.\n",
       "\n",
       "    .. versionadded:: 1.1\n",
       "\n",
       "support_ : ndarray of shape (n_SV)\n",
       "    Indices of support vectors.\n",
       "\n",
       "support_vectors_ : ndarray of shape (n_SV, n_features)\n",
       "    Support vectors.\n",
       "\n",
       "n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
       "    Number of support vectors for each class.\n",
       "\n",
       "probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
       "probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
       "    If `probability=True`, it corresponds to the parameters learned in\n",
       "    Platt scaling to produce probability estimates from decision values.\n",
       "    If `probability=False`, it's an empty array. Platt scaling uses the\n",
       "    logistic function\n",
       "    ``1 / (1 + exp(decision_value * probA_ + probB_))``\n",
       "    where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n",
       "    more information on the multiclass case and training procedure see\n",
       "    section 8 of [1]_.\n",
       "\n",
       "shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
       "    Array dimensions of training vector ``X``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "SVR : Support Vector Machine for Regression implemented using libsvm.\n",
       "\n",
       "LinearSVC : Scalable Linear Support Vector Machine for classification\n",
       "    implemented using liblinear. Check the See Also section of\n",
       "    LinearSVC for more comparison element.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] `LIBSVM: A Library for Support Vector Machines\n",
       "    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
       "\n",
       ".. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n",
       "    machines and comparison to regularizedlikelihood methods.\"\n",
       "    <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.pipeline import make_pipeline\n",
       ">>> from sklearn.preprocessing import StandardScaler\n",
       ">>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
       ">>> y = np.array([1, 1, 2, 2])\n",
       ">>> from sklearn.svm import SVC\n",
       ">>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
       ">>> clf.fit(X, y)\n",
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])\n",
       "\n",
       ">>> print(clf.predict([[-0.8, -1]]))\n",
       "[1]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "SVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4277d341-87ab-4ec9-9f3b-1f86b2ddab21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'squared_hinge'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdual\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mintercept_scaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Linear Support Vector Classification.\n",
       "\n",
       "Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
       "liblinear rather than libsvm, so it has more flexibility in the choice of\n",
       "penalties and loss functions and should scale better to large numbers of\n",
       "samples.\n",
       "\n",
       "This class supports both dense and sparse input and the multiclass support\n",
       "is handled according to a one-vs-the-rest scheme.\n",
       "\n",
       "Read more in the :ref:`User Guide <svm_classification>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "penalty : {'l1', 'l2'}, default='l2'\n",
       "    Specifies the norm used in the penalization. The 'l2'\n",
       "    penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
       "    vectors that are sparse.\n",
       "\n",
       "loss : {'hinge', 'squared_hinge'}, default='squared_hinge'\n",
       "    Specifies the loss function. 'hinge' is the standard SVM loss\n",
       "    (used e.g. by the SVC class) while 'squared_hinge' is the\n",
       "    square of the hinge loss. The combination of ``penalty='l1'``\n",
       "    and ``loss='hinge'`` is not supported.\n",
       "\n",
       "dual : bool, default=True\n",
       "    Select the algorithm to either solve the dual or primal\n",
       "    optimization problem. Prefer dual=False when n_samples > n_features.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for stopping criteria.\n",
       "\n",
       "C : float, default=1.0\n",
       "    Regularization parameter. The strength of the regularization is\n",
       "    inversely proportional to C. Must be strictly positive.\n",
       "\n",
       "multi_class : {'ovr', 'crammer_singer'}, default='ovr'\n",
       "    Determines the multi-class strategy if `y` contains more than\n",
       "    two classes.\n",
       "    ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
       "    ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
       "    While `crammer_singer` is interesting from a theoretical perspective\n",
       "    as it is consistent, it is seldom used in practice as it rarely leads\n",
       "    to better accuracy and is more expensive to compute.\n",
       "    If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
       "    will be ignored.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Whether to calculate the intercept for this model. If set\n",
       "    to false, no intercept will be used in calculations\n",
       "    (i.e. data is expected to be already centered).\n",
       "\n",
       "intercept_scaling : float, default=1\n",
       "    When self.fit_intercept is True, instance vector x becomes\n",
       "    ``[x, self.intercept_scaling]``,\n",
       "    i.e. a \"synthetic\" feature with constant value equals to\n",
       "    intercept_scaling is appended to the instance vector.\n",
       "    The intercept becomes intercept_scaling * synthetic feature weight\n",
       "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
       "    as all other features.\n",
       "    To lessen the effect of regularization on synthetic feature weight\n",
       "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
       "\n",
       "class_weight : dict or 'balanced', default=None\n",
       "    Set the parameter C of class i to ``class_weight[i]*C`` for\n",
       "    SVC. If not given, all classes are supposed to have\n",
       "    weight one.\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Enable verbose output. Note that this setting takes advantage of a\n",
       "    per-process runtime setting in liblinear that, if enabled, may not work\n",
       "    properly in a multithreaded context.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the pseudo random number generation for shuffling the data for\n",
       "    the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\n",
       "    underlying implementation of :class:`LinearSVC` is not random and\n",
       "    ``random_state`` has no effect on the results.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "max_iter : int, default=1000\n",
       "    The maximum number of iterations to be run.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\n",
       "    Weights assigned to the features (coefficients in the primal\n",
       "    problem).\n",
       "\n",
       "    ``coef_`` is a readonly property derived from ``raw_coef_`` that\n",
       "    follows the internal memory layout of liblinear.\n",
       "\n",
       "intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
       "    Constants in decision function.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    The unique classes labels.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_iter_ : int\n",
       "    Maximum number of iterations run across all classes.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "SVC : Implementation of Support Vector Machine classifier using libsvm:\n",
       "    the kernel can be non-linear but its SMO algorithm does not\n",
       "    scale to large number of samples as LinearSVC does.\n",
       "\n",
       "    Furthermore SVC multi-class mode is implemented using one\n",
       "    vs one scheme while LinearSVC uses one vs the rest. It is\n",
       "    possible to implement one vs the rest with SVC by using the\n",
       "    :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
       "\n",
       "    Finally SVC can fit dense data without memory copy if the input\n",
       "    is C-contiguous. Sparse data will still incur memory copy though.\n",
       "\n",
       "sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same\n",
       "    cost function as LinearSVC\n",
       "    by adjusting the penalty and loss parameters. In addition it requires\n",
       "    less memory, allows incremental (online) learning, and implements\n",
       "    various loss functions and regularization regimes.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The underlying C implementation uses a random number generator to\n",
       "select features when fitting the model. It is thus not uncommon\n",
       "to have slightly different results for the same input data. If\n",
       "that happens, try with a smaller ``tol`` parameter.\n",
       "\n",
       "The underlying implementation, liblinear, uses a sparse internal\n",
       "representation for the data that will incur a memory copy.\n",
       "\n",
       "Predict output may not match that of standalone liblinear in certain\n",
       "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
       "in the narrative documentation.\n",
       "\n",
       "References\n",
       "----------\n",
       "`LIBLINEAR: A Library for Large Linear Classification\n",
       "<https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.svm import LinearSVC\n",
       ">>> from sklearn.pipeline import make_pipeline\n",
       ">>> from sklearn.preprocessing import StandardScaler\n",
       ">>> from sklearn.datasets import make_classification\n",
       ">>> X, y = make_classification(n_features=4, random_state=0)\n",
       ">>> clf = make_pipeline(StandardScaler(),\n",
       "...                     LinearSVC(random_state=0, tol=1e-5))\n",
       ">>> clf.fit(X, y)\n",
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n",
       "\n",
       ">>> print(clf.named_steps['linearsvc'].coef_)\n",
       "[[0.141...   0.526... 0.679... 0.493...]]\n",
       "\n",
       ">>> print(clf.named_steps['linearsvc'].intercept_)\n",
       "[0.1693...]\n",
       ">>> print(clf.predict([[0, 0, 0, 0]]))\n",
       "[1]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\svm\\_classes.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LinearSVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb6f52f-19b5-4855-b0af-242712799c6e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdual\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mintercept_scaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Logistic Regression (aka logit, MaxEnt) classifier.\n",
       "\n",
       "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
       "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
       "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
       "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
       "'sag', 'saga' and 'newton-cg' solvers.)\n",
       "\n",
       "This class implements regularized logistic regression using the\n",
       "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
       "that regularization is applied by default**. It can handle both dense\n",
       "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
       "floats for optimal performance; any other input format will be converted\n",
       "(and copied).\n",
       "\n",
       "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
       "with primal formulation, or no regularization. The 'liblinear' solver\n",
       "supports both L1 and L2 regularization, with a dual formulation only for\n",
       "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
       "'saga' solver.\n",
       "\n",
       "Read more in the :ref:`User Guide <logistic_regression>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
       "    Specify the norm of the penalty:\n",
       "\n",
       "    - `'none'`: no penalty is added;\n",
       "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
       "    - `'l1'`: add a L1 penalty term;\n",
       "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
       "\n",
       "    .. warning::\n",
       "       Some penalties may not work with some solvers. See the parameter\n",
       "       `solver` below, to know the compatibility between the penalty and\n",
       "       solver.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
       "\n",
       "dual : bool, default=False\n",
       "    Dual or primal formulation. Dual formulation is only implemented for\n",
       "    l2 penalty with liblinear solver. Prefer dual=False when\n",
       "    n_samples > n_features.\n",
       "\n",
       "tol : float, default=1e-4\n",
       "    Tolerance for stopping criteria.\n",
       "\n",
       "C : float, default=1.0\n",
       "    Inverse of regularization strength; must be a positive float.\n",
       "    Like in support vector machines, smaller values specify stronger\n",
       "    regularization.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
       "    added to the decision function.\n",
       "\n",
       "intercept_scaling : float, default=1\n",
       "    Useful only when the solver 'liblinear' is used\n",
       "    and self.fit_intercept is set to True. In this case, x becomes\n",
       "    [x, self.intercept_scaling],\n",
       "    i.e. a \"synthetic\" feature with constant value equal to\n",
       "    intercept_scaling is appended to the instance vector.\n",
       "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
       "\n",
       "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
       "    as all other features.\n",
       "    To lessen the effect of regularization on synthetic feature weight\n",
       "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
       "\n",
       "class_weight : dict or 'balanced', default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If not given, all classes are supposed to have weight one.\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *class_weight='balanced'*\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
       "    data. See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
       "\n",
       "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
       "    To choose a solver, you might want to consider the following aspects:\n",
       "\n",
       "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
       "          and 'saga' are faster for large ones;\n",
       "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
       "          'lbfgs' handle multinomial loss;\n",
       "        - 'liblinear' is limited to one-versus-rest schemes.\n",
       "\n",
       "    .. warning::\n",
       "       The choice of the algorithm depends on the penalty chosen:\n",
       "       Supported penalties by solver:\n",
       "\n",
       "       - 'newton-cg'   -   ['l2', 'none']\n",
       "       - 'lbfgs'       -   ['l2', 'none']\n",
       "       - 'liblinear'   -   ['l1', 'l2']\n",
       "       - 'sag'         -   ['l2', 'none']\n",
       "       - 'saga'        -   ['elasticnet', 'l1', 'l2', 'none']\n",
       "\n",
       "    .. note::\n",
       "       'sag' and 'saga' fast convergence is only guaranteed on\n",
       "       features with approximately the same scale. You can\n",
       "       preprocess the data with a scaler from :mod:`sklearn.preprocessing`.\n",
       "\n",
       "    .. seealso::\n",
       "       Refer to the User Guide for more information regarding\n",
       "       :class:`LogisticRegression` and more specifically the\n",
       "       `Table <https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression>`_\n",
       "       summarazing solver/penalty supports.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       Stochastic Average Gradient descent solver.\n",
       "    .. versionadded:: 0.19\n",
       "       SAGA solver.\n",
       "    .. versionchanged:: 0.22\n",
       "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
       "\n",
       "max_iter : int, default=100\n",
       "    Maximum number of iterations taken for the solvers to converge.\n",
       "\n",
       "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
       "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
       "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
       "    across the entire probability distribution, *even when the data is\n",
       "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
       "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
       "    and otherwise selects 'multinomial'.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
       "    .. versionchanged:: 0.22\n",
       "        Default changed from 'ovr' to 'auto' in 0.22.\n",
       "\n",
       "verbose : int, default=0\n",
       "    For the liblinear and lbfgs solvers set verbose to any positive\n",
       "    number for verbosity.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to True, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    Number of CPU cores used when parallelizing over classes if\n",
       "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
       "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
       "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "    context. ``-1`` means using all processors.\n",
       "    See :term:`Glossary <n_jobs>` for more details.\n",
       "\n",
       "l1_ratio : float, default=None\n",
       "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
       "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
       "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
       "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
       "    combination of L1 and L2.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "\n",
       "classes_ : ndarray of shape (n_classes, )\n",
       "    A list of class labels known to the classifier.\n",
       "\n",
       "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
       "    Coefficient of the features in the decision function.\n",
       "\n",
       "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
       "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
       "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
       "\n",
       "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
       "    Intercept (a.k.a. bias) added to the decision function.\n",
       "\n",
       "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
       "    `intercept_` is of shape (1,) when the given problem is binary.\n",
       "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
       "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
       "    outcome 0 (False).\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
       "    Actual number of iterations for all classes. If binary or multinomial,\n",
       "    it returns only 1 element. For liblinear solver, only the maximum\n",
       "    number of iteration across all classes is given.\n",
       "\n",
       "    .. versionchanged:: 0.20\n",
       "\n",
       "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
       "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "SGDClassifier : Incrementally trained logistic regression (when given\n",
       "    the parameter ``loss=\"log\"``).\n",
       "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The underlying C implementation uses a random number generator to\n",
       "select features when fitting the model. It is thus not uncommon,\n",
       "to have slightly different results for the same input data. If\n",
       "that happens, try with a smaller tol parameter.\n",
       "\n",
       "Predict output may not match that of standalone liblinear in certain\n",
       "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
       "in the narrative documentation.\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
       "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
       "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
       "\n",
       "LIBLINEAR -- A Library for Large Linear Classification\n",
       "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
       "\n",
       "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
       "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
       "    https://hal.inria.fr/hal-00860051/document\n",
       "\n",
       "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
       "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
       "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
       "\n",
       "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
       "    methods for logistic regression and maximum entropy models.\n",
       "    Machine Learning 85(1-2):41-75.\n",
       "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.datasets import load_iris\n",
       ">>> from sklearn.linear_model import LogisticRegression\n",
       ">>> X, y = load_iris(return_X_y=True)\n",
       ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
       ">>> clf.predict(X[:2, :])\n",
       "array([0, 0])\n",
       ">>> clf.predict_proba(X[:2, :])\n",
       "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
       "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
       ">>> clf.score(X, y)\n",
       "0.97...\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     LogisticRegressionCV\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f56029e-44b7-437c-9be2-7bda701acd6c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mzero_division\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'warn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Compute the F1 score, also known as balanced F-score or F-measure.\n",
       "\n",
       "The F1 score can be interpreted as a harmonic mean of the precision and\n",
       "recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
       "The relative contribution of precision and recall to the F1 score are\n",
       "equal. The formula for the F1 score is::\n",
       "\n",
       "    F1 = 2 * (precision * recall) / (precision + recall)\n",
       "\n",
       "In the multi-class and multi-label case, this is the average of\n",
       "the F1 score of each class with weighting depending on the ``average``\n",
       "parameter.\n",
       "\n",
       "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : 1d array-like, or label indicator array / sparse matrix\n",
       "    Ground truth (correct) target values.\n",
       "\n",
       "y_pred : 1d array-like, or label indicator array / sparse matrix\n",
       "    Estimated targets as returned by a classifier.\n",
       "\n",
       "labels : array-like, default=None\n",
       "    The set of labels to include when ``average != 'binary'``, and their\n",
       "    order if ``average is None``. Labels present in the data can be\n",
       "    excluded, for example to calculate a multiclass average ignoring a\n",
       "    majority negative class, while labels not present in the data will\n",
       "    result in 0 components in a macro average. For multilabel targets,\n",
       "    labels are column indices. By default, all labels in ``y_true`` and\n",
       "    ``y_pred`` are used in sorted order.\n",
       "\n",
       "    .. versionchanged:: 0.17\n",
       "       Parameter `labels` improved for multiclass problem.\n",
       "\n",
       "pos_label : str or int, default=1\n",
       "    The class to report if ``average='binary'`` and the data is binary.\n",
       "    If the data are multiclass or multilabel, this will be ignored;\n",
       "    setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
       "    scores for that label only.\n",
       "\n",
       "average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
       "    This parameter is required for multiclass/multilabel targets.\n",
       "    If ``None``, the scores for each class are returned. Otherwise, this\n",
       "    determines the type of averaging performed on the data:\n",
       "\n",
       "    ``'binary'``:\n",
       "        Only report results for the class specified by ``pos_label``.\n",
       "        This is applicable only if targets (``y_{true,pred}``) are binary.\n",
       "    ``'micro'``:\n",
       "        Calculate metrics globally by counting the total true positives,\n",
       "        false negatives and false positives.\n",
       "    ``'macro'``:\n",
       "        Calculate metrics for each label, and find their unweighted\n",
       "        mean.  This does not take label imbalance into account.\n",
       "    ``'weighted'``:\n",
       "        Calculate metrics for each label, and find their average weighted\n",
       "        by support (the number of true instances for each label). This\n",
       "        alters 'macro' to account for label imbalance; it can result in an\n",
       "        F-score that is not between precision and recall.\n",
       "    ``'samples'``:\n",
       "        Calculate metrics for each instance, and find their average (only\n",
       "        meaningful for multilabel classification where this differs from\n",
       "        :func:`accuracy_score`).\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
       "    Sets the value to return when there is a zero division, i.e. when all\n",
       "    predictions and labels are negative. If set to \"warn\", this acts as 0,\n",
       "    but warnings are also raised.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "f1_score : float or array of float, shape = [n_unique_labels]\n",
       "    F1 score of the positive class in binary classification or weighted\n",
       "    average of the F1 scores of each class for the multiclass task.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "fbeta_score : Compute the F-beta score.\n",
       "precision_recall_fscore_support : Compute the precision, recall, F-score,\n",
       "    and support.\n",
       "jaccard_score : Compute the Jaccard similarity coefficient score.\n",
       "multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
       "    sample.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "When ``true positive + false positive == 0``, precision is undefined.\n",
       "When ``true positive + false negative == 0``, recall is undefined.\n",
       "In such cases, by default the metric will be set to 0, as will f-score,\n",
       "and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
       "modified with ``zero_division``.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] `Wikipedia entry for the F1-score\n",
       "       <https://en.wikipedia.org/wiki/F1_score>`_.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import f1_score\n",
       ">>> y_true = [0, 1, 2, 0, 1, 2]\n",
       ">>> y_pred = [0, 2, 1, 0, 0, 1]\n",
       ">>> f1_score(y_true, y_pred, average='macro')\n",
       "0.26...\n",
       ">>> f1_score(y_true, y_pred, average='micro')\n",
       "0.33...\n",
       ">>> f1_score(y_true, y_pred, average='weighted')\n",
       "0.26...\n",
       ">>> f1_score(y_true, y_pred, average=None)\n",
       "array([0.8, 0. , 0. ])\n",
       ">>> y_true = [0, 0, 0, 0, 0, 0]\n",
       ">>> y_pred = [0, 0, 0, 0, 0, 0]\n",
       ">>> f1_score(y_true, y_pred, zero_division=1)\n",
       "1.0...\n",
       ">>> # multilabel classification\n",
       ">>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
       ">>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
       ">>> f1_score(y_true, y_pred, average=None)\n",
       "array([0.66666667, 1.        , 0.66666667])\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\metrics\\_classification.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72052e9a-01eb-4606-b57f-1bbcebe8ec25",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Compute the weighted average along the specified axis.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "a : array_like\n",
       "    Array containing data to be averaged. If `a` is not an array, a\n",
       "    conversion is attempted.\n",
       "axis : None or int or tuple of ints, optional\n",
       "    Axis or axes along which to average `a`.  The default,\n",
       "    axis=None, will average over all of the elements of the input array.\n",
       "    If axis is negative it counts from the last to the first axis.\n",
       "\n",
       "    .. versionadded:: 1.7.0\n",
       "\n",
       "    If axis is a tuple of ints, averaging is performed on all of the axes\n",
       "    specified in the tuple instead of a single axis or all the axes as\n",
       "    before.\n",
       "weights : array_like, optional\n",
       "    An array of weights associated with the values in `a`. Each value in\n",
       "    `a` contributes to the average according to its associated weight.\n",
       "    The weights array can either be 1-D (in which case its length must be\n",
       "    the size of `a` along the given axis) or of the same shape as `a`.\n",
       "    If `weights=None`, then all data in `a` are assumed to have a\n",
       "    weight equal to one.  The 1-D calculation is::\n",
       "\n",
       "        avg = sum(a * weights) / sum(weights)\n",
       "\n",
       "    The only constraint on `weights` is that `sum(weights)` must not be 0.\n",
       "returned : bool, optional\n",
       "    Default is `False`. If `True`, the tuple (`average`, `sum_of_weights`)\n",
       "    is returned, otherwise only the average is returned.\n",
       "    If `weights=None`, `sum_of_weights` is equivalent to the number of\n",
       "    elements over which the average is taken.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "retval, [sum_of_weights] : array_type or double\n",
       "    Return the average along the specified axis. When `returned` is `True`,\n",
       "    return a tuple with the average as the first element and the sum\n",
       "    of the weights as the second element. `sum_of_weights` is of the\n",
       "    same type as `retval`. The result dtype follows a genereal pattern.\n",
       "    If `weights` is None, the result dtype will be that of `a` , or ``float64``\n",
       "    if `a` is integral. Otherwise, if `weights` is not None and `a` is non-\n",
       "    integral, the result type will be the type of lowest precision capable of\n",
       "    representing values of both `a` and `weights`. If `a` happens to be\n",
       "    integral, the previous rules still applies but the result dtype will\n",
       "    at least be ``float64``.\n",
       "\n",
       "Raises\n",
       "------\n",
       "ZeroDivisionError\n",
       "    When all weights along axis are zero. See `numpy.ma.average` for a\n",
       "    version robust to this type of error.\n",
       "TypeError\n",
       "    When the length of 1D `weights` is not the same as the shape of `a`\n",
       "    along axis.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "mean\n",
       "\n",
       "ma.average : average for masked arrays -- useful if your data contains\n",
       "             \"missing\" values\n",
       "numpy.result_type : Returns the type that results from applying the\n",
       "                    numpy type promotion rules to the arguments.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> data = np.arange(1, 5)\n",
       ">>> data\n",
       "array([1, 2, 3, 4])\n",
       ">>> np.average(data)\n",
       "2.5\n",
       ">>> np.average(np.arange(1, 11), weights=np.arange(10, 0, -1))\n",
       "4.0\n",
       "\n",
       ">>> data = np.arange(6).reshape((3,2))\n",
       ">>> data\n",
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])\n",
       ">>> np.average(data, axis=1, weights=[1./4, 3./4])\n",
       "array([0.75, 2.75, 4.75])\n",
       ">>> np.average(data, weights=[1./4, 3./4])\n",
       "Traceback (most recent call last):\n",
       "    ...\n",
       "TypeError: Axis must be specified when shapes of a and weights differ.\n",
       "\n",
       ">>> a = np.ones(5, dtype=np.float128)\n",
       ">>> w = np.ones(5, dtype=np.complex64)\n",
       ">>> avg = np.average(a, weights=w)\n",
       ">>> print(avg.dtype)\n",
       "complex256\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\numpy\\lib\\function_base.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6b69d9-5cd1-43a7-b572-bd8342d8a55c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gini'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sqrt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moob_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "A random forest classifier.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of decision tree\n",
       "classifiers on various sub-samples of the dataset and uses averaging to\n",
       "improve the predictive accuracy and control over-fitting.\n",
       "The sub-sample size is controlled with the `max_samples` parameter if\n",
       "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
       "each tree.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_estimators : int, default=100\n",
       "    The number of trees in the forest.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``n_estimators`` changed from 10 to 100\n",
       "       in 0.22.\n",
       "\n",
       "criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
       "    The function to measure the quality of a split. Supported criteria are\n",
       "    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
       "    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
       "    Note: This parameter is tree-specific.\n",
       "\n",
       "max_depth : int, default=None\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, then consider `max_features` features at each split.\n",
       "    - If float, then `max_features` is a fraction and\n",
       "      `round(max_features * n_features)` features are considered at each\n",
       "      split.\n",
       "    - If \"auto\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    .. versionchanged:: 1.1\n",
       "        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n",
       "\n",
       "    .. deprecated:: 1.1\n",
       "        The `\"auto\"` option was deprecated in 1.1 and will be removed\n",
       "        in 1.3.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "bootstrap : bool, default=True\n",
       "    Whether bootstrap samples are used when building trees. If False, the\n",
       "    whole dataset is used to build each tree.\n",
       "\n",
       "oob_score : bool, default=False\n",
       "    Whether to use out-of-bag samples to estimate the generalization score.\n",
       "    Only available if bootstrap=True.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
       "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
       "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "    context. ``-1`` means using all processors. See :term:`Glossary\n",
       "    <n_jobs>` for more details.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls both the randomness of the bootstrapping of the samples used\n",
       "    when building trees (if ``bootstrap=True``) and the sampling of the\n",
       "    features to consider when looking for the best split at each node\n",
       "    (if ``max_features < n_features``).\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Controls the verbosity when fitting and predicting.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
       "    new forest. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If not given, all classes are supposed to have weight one. For\n",
       "    multi-output problems, a list of dicts can be provided in the same\n",
       "    order as the columns of y.\n",
       "\n",
       "    Note that for multioutput (including multilabel) weights should be\n",
       "    defined for each class of every column in its own dict. For example,\n",
       "    for four-class multilabel classification weights should be\n",
       "    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
       "    [{1:1}, {2:5}, {3:1}, {4:1}].\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``\n",
       "\n",
       "    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
       "    weights are computed based on the bootstrap sample for every tree\n",
       "    grown.\n",
       "\n",
       "    For multi-output, the weights of each column of y will be multiplied.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "max_samples : int or float, default=None\n",
       "    If bootstrap is True, the number of samples to draw from X\n",
       "    to train each base estimator.\n",
       "\n",
       "    - If None (default), then draw `X.shape[0]` samples.\n",
       "    - If int, then draw `max_samples` samples.\n",
       "    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
       "      `max_samples` should be in the interval `(0.0, 1.0]`.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "base_estimator_ : DecisionTreeClassifier\n",
       "    The child estimator template used to create the collection of fitted\n",
       "    sub-estimators.\n",
       "\n",
       "estimators_ : list of DecisionTreeClassifier\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
       "    The classes labels (single output problem), or a list of arrays of\n",
       "    class labels (multi-output problem).\n",
       "\n",
       "n_classes_ : int or list\n",
       "    The number of classes (single output problem), or a list containing the\n",
       "    number of classes for each output (multi-output problem).\n",
       "\n",
       "n_features_ : int\n",
       "    The number of features when ``fit`` is performed.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        Attribute `n_features_` was deprecated in version 1.0 and will be\n",
       "        removed in 1.2. Use `n_features_in_` instead.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs when ``fit`` is performed.\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "oob_score_ : float\n",
       "    Score of the training dataset obtained using an out-of-bag estimate.\n",
       "    This attribute exists only when ``oob_score`` is True.\n",
       "\n",
       "oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
       "    Decision function computed with out-of-bag estimate on the training\n",
       "    set. If n_estimators is small it might be possible that a data point\n",
       "    was never left out during the bootstrap. In this case,\n",
       "    `oob_decision_function_` might contain NaN. This attribute exists\n",
       "    only when ``oob_score`` is True.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
       "sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
       "    tree classifiers.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data,\n",
       "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
       "of the criterion is identical for several splits enumerated during the\n",
       "search of the best split. To obtain a deterministic behaviour during\n",
       "fitting, ``random_state`` has to be fixed.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.ensemble import RandomForestClassifier\n",
       ">>> from sklearn.datasets import make_classification\n",
       ">>> X, y = make_classification(n_samples=1000, n_features=4,\n",
       "...                            n_informative=2, n_redundant=0,\n",
       "...                            random_state=0, shuffle=False)\n",
       ">>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
       ">>> clf.fit(X, y)\n",
       "RandomForestClassifier(...)\n",
       ">>> print(clf.predict([[0, 0, 0, 0]]))\n",
       "[1]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80638cc-c731-47af-9180-9ef4c92162c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hinge'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'optimal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0meta0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpower_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mearly_stopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_iter_no_change\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
       "\n",
       "This estimator implements regularized linear models with stochastic\n",
       "gradient descent (SGD) learning: the gradient of the loss is estimated\n",
       "each sample at a time and the model is updated along the way with a\n",
       "decreasing strength schedule (aka learning rate). SGD allows minibatch\n",
       "(online/out-of-core) learning via the `partial_fit` method.\n",
       "For best results using the default learning rate schedule, the data should\n",
       "have zero mean and unit variance.\n",
       "\n",
       "This implementation works with data represented as dense or sparse arrays\n",
       "of floating point values for the features. The model it fits can be\n",
       "controlled with the loss parameter; by default, it fits a linear support\n",
       "vector machine (SVM).\n",
       "\n",
       "The regularizer is a penalty added to the loss function that shrinks model\n",
       "parameters towards the zero vector using either the squared euclidean norm\n",
       "L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
       "parameter update crosses the 0.0 value because of the regularizer, the\n",
       "update is truncated to 0.0 to allow for learning sparse models and achieve\n",
       "online feature selection.\n",
       "\n",
       "Read more in the :ref:`User Guide <sgd>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "loss : {'hinge', 'log_loss', 'log', 'modified_huber', 'squared_hinge',        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',        'squared_epsilon_insensitive'}, default='hinge'\n",
       "    The loss function to be used.\n",
       "\n",
       "    - 'hinge' gives a linear SVM.\n",
       "    - 'log_loss' gives logistic regression, a probabilistic classifier.\n",
       "    - 'modified_huber' is another smooth loss that brings tolerance to\n",
       "       outliers as well as probability estimates.\n",
       "    - 'squared_hinge' is like hinge but is quadratically penalized.\n",
       "    - 'perceptron' is the linear loss used by the perceptron algorithm.\n",
       "    - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and\n",
       "      'squared_epsilon_insensitive' are designed for regression but can be useful\n",
       "      in classification as well; see\n",
       "      :class:`~sklearn.linear_model.SGDRegressor` for a description.\n",
       "\n",
       "    More details about the losses formulas can be found in the\n",
       "    :ref:`User Guide <sgd_mathematical_formulation>`.\n",
       "\n",
       "    .. deprecated:: 1.0\n",
       "        The loss 'squared_loss' was deprecated in v1.0 and will be removed\n",
       "        in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
       "\n",
       "    .. deprecated:: 1.1\n",
       "        The loss 'log' was deprecated in v1.1 and will be removed\n",
       "        in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
       "\n",
       "penalty : {'l2', 'l1', 'elasticnet'}, default='l2'\n",
       "    The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
       "    which is the standard regularizer for linear SVM models. 'l1' and\n",
       "    'elasticnet' might bring sparsity to the model (feature selection)\n",
       "    not achievable with 'l2'.\n",
       "\n",
       "alpha : float, default=0.0001\n",
       "    Constant that multiplies the regularization term. The higher the\n",
       "    value, the stronger the regularization.\n",
       "    Also used to compute the learning rate when set to `learning_rate` is\n",
       "    set to 'optimal'.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "\n",
       "l1_ratio : float, default=0.15\n",
       "    The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
       "    l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
       "    Only used if `penalty` is 'elasticnet'.\n",
       "    Values must be in the range `[0.0, 1.0]`.\n",
       "\n",
       "fit_intercept : bool, default=True\n",
       "    Whether the intercept should be estimated or not. If False, the\n",
       "    data is assumed to be already centered.\n",
       "\n",
       "max_iter : int, default=1000\n",
       "    The maximum number of passes over the training data (aka epochs).\n",
       "    It only impacts the behavior in the ``fit`` method, and not the\n",
       "    :meth:`partial_fit` method.\n",
       "    Values must be in the range `[1, inf)`.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "tol : float, default=1e-3\n",
       "    The stopping criterion. If it is not None, training will stop\n",
       "    when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
       "    epochs.\n",
       "    Convergence is checked against the training loss or the\n",
       "    validation loss depending on the `early_stopping` parameter.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "shuffle : bool, default=True\n",
       "    Whether or not the training data should be shuffled after each epoch.\n",
       "\n",
       "verbose : int, default=0\n",
       "    The verbosity level.\n",
       "    Values must be in the range `[0, inf)`.\n",
       "\n",
       "epsilon : float, default=0.1\n",
       "    Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
       "    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
       "    For 'huber', determines the threshold at which it becomes less\n",
       "    important to get the prediction exactly right.\n",
       "    For epsilon-insensitive, any differences between the current prediction\n",
       "    and the correct label are ignored if they are less than this threshold.\n",
       "    Values must be in the range `[0.0, inf)`.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of CPUs to use to do the OVA (One Versus All, for\n",
       "    multi-class problems) computation.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "random_state : int, RandomState instance, default=None\n",
       "    Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "    Integer values must be in the range `[0, 2**32 - 1]`.\n",
       "\n",
       "learning_rate : str, default='optimal'\n",
       "    The learning rate schedule:\n",
       "\n",
       "    - 'constant': `eta = eta0`\n",
       "    - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
       "      where `t0` is chosen by a heuristic proposed by Leon Bottou.\n",
       "    - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
       "    - 'adaptive': `eta = eta0`, as long as the training keeps decreasing.\n",
       "      Each time n_iter_no_change consecutive epochs fail to decrease the\n",
       "      training loss by tol or fail to increase validation score by tol if\n",
       "      `early_stopping` is `True`, the current learning rate is divided by 5.\n",
       "\n",
       "        .. versionadded:: 0.20\n",
       "            Added 'adaptive' option\n",
       "\n",
       "eta0 : float, default=0.0\n",
       "    The initial learning rate for the 'constant', 'invscaling' or\n",
       "    'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n",
       "    the default schedule 'optimal'.\n",
       "    Values must be in the range `(0.0, inf)`.\n",
       "\n",
       "power_t : float, default=0.5\n",
       "    The exponent for inverse scaling learning rate [default 0.5].\n",
       "    Values must be in the range `(-inf, inf)`.\n",
       "\n",
       "early_stopping : bool, default=False\n",
       "    Whether to use early stopping to terminate training when validation\n",
       "    score is not improving. If set to `True`, it will automatically set aside\n",
       "    a stratified fraction of training data as validation and terminate\n",
       "    training when validation score returned by the `score` method is not\n",
       "    improving by at least tol for n_iter_no_change consecutive epochs.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "        Added 'early_stopping' option\n",
       "\n",
       "validation_fraction : float, default=0.1\n",
       "    The proportion of training data to set aside as validation set for\n",
       "    early stopping. Must be between 0 and 1.\n",
       "    Only used if `early_stopping` is True.\n",
       "    Values must be in the range `(0.0, 1.0)`.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "        Added 'validation_fraction' option\n",
       "\n",
       "n_iter_no_change : int, default=5\n",
       "    Number of iterations with no improvement to wait before stopping\n",
       "    fitting.\n",
       "    Convergence is checked against the training loss or the\n",
       "    validation loss depending on the `early_stopping` parameter.\n",
       "    Integer values must be in the range `[1, max_iter)`.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "        Added 'n_iter_no_change' option\n",
       "\n",
       "class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
       "    Preset for the class_weight fit parameter.\n",
       "\n",
       "    Weights associated with classes. If not given, all classes\n",
       "    are supposed to have weight one.\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to True, reuse the solution of the previous call to fit as\n",
       "    initialization, otherwise, just erase the previous solution.\n",
       "    See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "    Repeatedly calling fit or partial_fit when warm_start is True can\n",
       "    result in a different solution than when calling fit a single time\n",
       "    because of the way the data is shuffled.\n",
       "    If a dynamic learning rate is used, the learning rate is adapted\n",
       "    depending on the number of samples already seen. Calling ``fit`` resets\n",
       "    this counter, while ``partial_fit`` will result in increasing the\n",
       "    existing counter.\n",
       "\n",
       "average : bool or int, default=False\n",
       "    When set to `True`, computes the averaged SGD weights across all\n",
       "    updates and stores the result in the ``coef_`` attribute. If set to\n",
       "    an int greater than 1, averaging will begin once the total number of\n",
       "    samples seen reaches `average`. So ``average=10`` will begin\n",
       "    averaging after seeing 10 samples.\n",
       "    Integer values must be in the range `[1, n_samples]`.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
       "    Weights assigned to the features.\n",
       "\n",
       "intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
       "    Constants in decision function.\n",
       "\n",
       "n_iter_ : int\n",
       "    The actual number of iterations before reaching the stopping criterion.\n",
       "    For multiclass fits, it is the maximum over every binary fit.\n",
       "\n",
       "loss_function_ : concrete ``LossFunction``\n",
       "\n",
       "classes_ : array of shape (n_classes,)\n",
       "\n",
       "t_ : int\n",
       "    Number of weight updates performed during training.\n",
       "    Same as ``(n_iter_ * n_samples)``.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "See Also\n",
       "--------\n",
       "sklearn.svm.LinearSVC : Linear support vector classification.\n",
       "LogisticRegression : Logistic regression.\n",
       "Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n",
       "    ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n",
       "    penalty=None)``.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.linear_model import SGDClassifier\n",
       ">>> from sklearn.preprocessing import StandardScaler\n",
       ">>> from sklearn.pipeline import make_pipeline\n",
       ">>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
       ">>> Y = np.array([1, 1, 2, 2])\n",
       ">>> # Always scale the input. The most convenient way is to use a pipeline.\n",
       ">>> clf = make_pipeline(StandardScaler(),\n",
       "...                     SGDClassifier(max_iter=1000, tol=1e-3))\n",
       ">>> clf.fit(X, Y)\n",
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('sgdclassifier', SGDClassifier())])\n",
       ">>> print(clf.predict([[-0.8, -1]]))\n",
       "[1]\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\asus\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "SGDClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad56fc59-b32d-48bf-8d7f-f7214f1774bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
