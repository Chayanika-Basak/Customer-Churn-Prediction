\begin{tabular}{lrlllll}
 & S. No. & Objective & Dataset & Parameters & Result & Conclusion and Future Scope \\
0 & 1 & Customer churn prediction using improved balanced random forests & A major Chinese bank provided the database for this study. The dataset includes records of more than 20,000 customers described by 27 variables. & Demographics category: age, education, size of disposable income, employment type, marital status, number of dependants, and service grade. Account level category: account type, guarantee type, length of maturity of loan, loan data and loan amount. Customer behavior category: account status, credit status, and the number of times the terms of an agreement have been broken. & The top-decile lift of IBRF is better than that of ANN, DT, and CWC-SVM. IBRF achieves better performance than other algorithms because the distribution of different classes in training dataset for each iteration is designed to be relative balance. &  IBRF offers great potential compared to traditional approaches due to their scalability, and faster training and running speeds. Continuing research should aim at improving the effectiveness and generalization ability. IBRF employs internal variables to determine the distribution of samples. Although the results are found to be insensitive to the values of these variables, imposing some limitations on them in future experiments may enhance the predictive effectiveness of the method. I \\
1 & 2 & A Customer Churn Prediction using Pearson
Correlation Function and K Nearest Neighbor
Algorithm for Telecommunication Industry & Public dataset from Kaggle dataset (https://www.kaggle.com/blastchar/telco-customer-churn) in CSV format and specifically stated as telco customer churn data. The dataset consists 7043 rows with 21 attributes. & customerID, gender, SeniorCitizen, Partner, Dependents, Tenure, PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies, Contract, PaperlessBilling, PaymentMethod, MonthlyCharges, TotalCharges, Churn                & The churn prediction using SVM
generates 83.67%, KNN 80.45% and Random Forest 76.36%. During the testing
phase, the best accuracy is achieved by KNN algorithm with the result 97.78%,
while the two algorithms is achieved below 80%, where Random Forest 76.85%
and SVM 79.41%. & The comparison of several classifiers will help to accurately predict customer churn as well as address the main factor that leads to customer retention. Based on the results and evaluation, it was observed that the KNN algorithm outperforms the others with the accuracy for
training is 80.45% testing 97.78%. \\
2 & 3 & 
Machine Learning Based Customer Churn Prediction In Banking & The dataset used in this analysis was obtained from Kaggle to model churns. The dataset includes information of 10000 bank clients, and the target parameter is a binary variable that represents whether the customer has left the bank or still a customer. & Row number, Customer Id, Surname, Credit Score, Geography,  Gender, Age, Tenure, Balance, Num of Products, Has Cr Card, Is Active Member, Estimated Salary, Exited & that the DT and RF classifiers accuracy increased after oversampling, but there is no change in KNN accuracy with regard to oversampling and SVM accuracy reduced with oversampling, this indicates that SVM is not suitable for huge amounts of data.  the accuracy of KNN is increased compared to KNN without MRMR. The SVM accuracy is almost similar to SVM without MRMR, the DT and RF accuracies are decreased a little bit compared to previous models. & By oversampling, both of these headaches up to a certain degree can be resolved. The model examined KNN, SVM, Decision Tree, RF classifiers under different conditions for this study. A better result is achieved when using the RF classifier together with oversampling(95.74%). Feature selection methods have nothing to do with tree classifiers(Decision Tree and RF). As the result indicates, feature reduction(feature selection) is decreasing the prediction score of tree classifiers. Another observation is that unlike other classifiers, in SVM, oversampling is decreasing the score. Itâ€™s because the Bank dataset is lopsided. Hence, SVM unable to handle the data
well enough. \\
\end{tabular}
